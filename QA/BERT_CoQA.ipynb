{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkNWXYXpplFu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!mkdir ./data\n",
        "!wget https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json -P ./data\n",
        "!wget https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json -P ./data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from functools import partial\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import spacy\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from tqdm import tqdm\n",
        "import toml"
      ],
      "metadata": {
        "id": "IW7eeVDJr6pu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational question answering with BERT-base\n",
        "\n",
        "In this module we will explore the Conversational Question Answering task (CoQA) with the BERT-base model. The dataset contains a context paragraph, and several questions that can be asked based on the context paragraph. The answer to one question may depend on previous questions or answers, thus making it conversational."
      ],
      "metadata": {
        "id": "RpMxr-CYpuMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making a few objects\n",
        "In the preliminary steps we are making three objects to store a CoQA example, CoQA features (vectors) and Results (outputs)."
      ],
      "metadata": {
        "id": "fEVyEqCMq6_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CoqaExample(object):\n",
        "    \"\"\"Single CoQA example\"\"\"\n",
        "    def __init__( self, qas_id, question_text, doc_tokens, orig_answer_text=None, start_position=None, end_position=None,\n",
        "                 rational_start_position=None, rational_end_position=None, additional_answers=None,):\n",
        "        self.qas_id = qas_id\n",
        "        self.question_text = question_text\n",
        "        self.doc_tokens = doc_tokens\n",
        "        self.orig_answer_text = orig_answer_text\n",
        "        self.start_position = start_position\n",
        "        self.end_position = end_position\n",
        "        self.additional_answers = additional_answers\n",
        "        self.rational_start_position = rational_start_position\n",
        "        self.rational_end_position = rational_end_position\n",
        "    def __repr__(self):\n",
        "        return f\"\"\"Question : {self.question_text}\n",
        "                   Document : {self.doc_tokens}\n",
        "                   Ground Truth : {self.orig_answer_text}\n",
        "                   Answer Span : {self.start_position} -> {self.end_position}\"\"\"\n",
        "\n",
        "class CoqaFeatures(object):\n",
        "    \"\"\"Single CoQA feature\"\"\"\n",
        "    def __init__(self, unique_id, example_index, doc_span_index, tokens, token_to_orig_map, token_is_max_context, input_ids,\n",
        "                 input_mask, segment_ids, start_position=None, end_position=None, cls_idx=None, rational_mask=None):\n",
        "        self.unique_id = unique_id\n",
        "        self.example_index = example_index\n",
        "        self.doc_span_index = doc_span_index\n",
        "        self.tokens = tokens\n",
        "        self.token_to_orig_map = token_to_orig_map\n",
        "        self.token_is_max_context = token_is_max_context\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.start_position = start_position\n",
        "        self.end_position = end_position\n",
        "        self.cls_idx = cls_idx\n",
        "        self.rational_mask = rational_mask\n",
        "\n",
        "class Result(object):\n",
        "    \"\"\"Single result \"\"\"\n",
        "    def __init__(self, unique_id, start_logits, end_logits, yes_logits, no_logits, unk_logits):\n",
        "        self.unique_id = unique_id\n",
        "        self.start_logits = start_logits\n",
        "        self.end_logits = end_logits\n",
        "        self.yes_logits = yes_logits\n",
        "        self.no_logits = no_logits\n",
        "        self.unk_logits = unk_logits"
      ],
      "metadata": {
        "id": "77jIQwm9q7Gj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next we define the preprocessing step"
      ],
      "metadata": {
        "id": "CwOG2jlAsckn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Processor:\n",
        "    train_file = \"/content/data/coqa-train-v1.0.json\"\n",
        "    dev_file = \"/content/data/coqa-dev-v1.0.json\"\n",
        "    def is_whitespace(self, c):\n",
        "        \"\"\" Clean whitespaces \"\"\"\n",
        "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "            return True\n",
        "        return False\n",
        "    def _str(self, s):\n",
        "        \"\"\" Convert PTB tokens to normal tokens \"\"\"\n",
        "        if (s.lower() == '-lrb-'):\n",
        "            s = '('\n",
        "        elif (s.lower() == '-rrb-'):\n",
        "            s = ')'\n",
        "        elif (s.lower() == '-lsb-'):\n",
        "            s = '['\n",
        "        elif (s.lower() == '-rsb-'):\n",
        "            s = ']'\n",
        "        elif (s.lower() == '-lcb-'):\n",
        "            s = '{'\n",
        "        elif (s.lower() == '-rcb-'):\n",
        "            s = '}'\n",
        "        return s\n",
        "    def space_extend(self, matchobj):\n",
        "        return ' ' + matchobj.group(0) + ' '\n",
        "    def pre_proc(self, text):\n",
        "        text = re.sub(u'-|\\u2010|\\u2011|\\u2012|\\u2013|\\u2014|\\u2015|%|\\[|\\]|:|\\(|\\)|/|\\t', self.space_extend, text)\n",
        "        text = text.strip(' \\n')\n",
        "        text = re.sub('\\s+', ' ', text)\n",
        "        return text\n",
        "    def process(self, parsed_text):\n",
        "        output = {'word': [], 'offsets': [], 'sentences': []}\n",
        "        for token in parsed_text:\n",
        "            output['word'].append(self._str(token.text))\n",
        "            output['offsets'].append((token.idx, token.idx + len(token.text)))\n",
        "        word_idx = 0\n",
        "        for sent in parsed_text.sents:\n",
        "            output['sentences'].append((word_idx, word_idx + len(sent)))\n",
        "            word_idx += len(sent)\n",
        "        assert word_idx == len(output['word'])\n",
        "        return output\n",
        "    def get_raw_context_offsets(self, words, raw_text):\n",
        "        \"\"\" Find word boundaries for the context \"\"\"\n",
        "        raw_context_offsets = []\n",
        "        p = 0\n",
        "        for token in words:\n",
        "            while p < len(raw_text) and re.match('\\s', raw_text[p]):\n",
        "                p += 1\n",
        "            if raw_text[p:p + len(token)] != token:\n",
        "                print('something is wrong! token', token, 'raw_text:', raw_text)\n",
        "            raw_context_offsets.append((p, p + len(token)))\n",
        "            p += len(token)\n",
        "        return raw_context_offsets\n",
        "    def find_span(self, offsets, start, end):\n",
        "        \"\"\" Find the span in word start/end from character start/end\"\"\"\n",
        "        start_index = -1\n",
        "        end_index = -1\n",
        "        for i, offset in enumerate(offsets):\n",
        "            if (start_index < 0) or (start >= offset[0]):\n",
        "                start_index = i\n",
        "            if (end_index < 0) and (end <= offset[1]):\n",
        "                end_index = i\n",
        "        return (start_index, end_index)\n",
        "    def normalize_answer(self, s):\n",
        "        \"\"\" some answers may only be different by articles like a, an, the punctuation, etc but otherwise be similar. Fix that\"\"\"\n",
        "        def remove_articles(text):\n",
        "            regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "            return re.sub(regex, ' ', text)\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.split())\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "    def find_span_with_gt(self, context, offsets, ground_truth):\n",
        "        \"\"\" Given the ground truth answer, find the best matching span in the context that matches the ground truth\"\"\"\n",
        "        best_f1 = 0.0\n",
        "        best_span = (len(offsets) - 1, len(offsets) - 1)\n",
        "        gt = self.normalize_answer(self.pre_proc(ground_truth)).split()\n",
        "        ls = [ i for i in range(len(offsets)) if context[offsets[i][0]:offsets[i][1]].lower() in gt ]\n",
        "        \"\"\" Check every possible span and check F1 score w.r.t. ground truth to pick best span\"\"\"\n",
        "        for i in range(len(ls)):\n",
        "            for j in range(i, len(ls)):\n",
        "                pred = self.normalize_answer(\n",
        "                    self.pre_proc( context[offsets[ls[i]][0]:offsets[ls[j]][1]])).split()\n",
        "                common = Counter(pred) & Counter(gt)\n",
        "                num_same = sum(common.values())\n",
        "                if num_same > 0:\n",
        "                    precision = 1.0 * num_same / len(pred)\n",
        "                    recall = 1.0 * num_same / len(gt)\n",
        "                    f1 = (2 * precision * recall) / (precision + recall)\n",
        "                    if f1 > best_f1:\n",
        "                        best_f1 = f1\n",
        "                        best_span = (ls[i], ls[j])\n",
        "        return best_span\n",
        "    def get_examples(self, evaluate, history_len = 2, threads=16):\n",
        "        \"\"\" Returns the training examples from the data directory.\n",
        "        This method basically wraps the next method and implements multi-processing.\"\"\"\n",
        "        filename = self.dev_file if evaluate else self.train_file\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as reader:\n",
        "            input_data = json.load(reader)[\"data\"]\n",
        "        threads = min(threads, cpu_count())\n",
        "        with Pool(threads) as p:\n",
        "            annotate_ = partial(self._create_examples, history_len=history_len)\n",
        "            examples = list(tqdm( p.imap(annotate_, input_data), total=len(input_data), desc=\"Preprocessing examples\",))\n",
        "        examples = [item for sublist in examples for item in sublist]\n",
        "        return examples\n",
        "    def _create_examples(self, input_data, history_len):\n",
        "        \"\"\" Primary pre-processing procedure \"\"\"\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        examples = []\n",
        "        datum = input_data\n",
        "        context_str = datum['story']\n",
        "        _datum = {'context': context_str, 'source': datum['source'], 'id': datum['id'], 'filename': datum['filename']}\n",
        "        nlp_context = nlp(self.pre_proc(context_str))\n",
        "        _datum['annotated_context'] = self.process(nlp_context)\n",
        "        _datum['raw_context_offsets'] = self.get_raw_context_offsets(_datum['annotated_context']['word'], context_str)\n",
        "        assert len(datum['questions']) == len(datum['answers'])\n",
        "        additional_answers = {}\n",
        "        if 'additional_answers' in datum:\n",
        "            for k, answer in datum['additional_answers'].items():\n",
        "                if len(answer) == len(datum['answers']):\n",
        "                    for ex in answer:\n",
        "                        idx = ex['turn_id']\n",
        "                        if idx not in additional_answers:\n",
        "                            additional_answers[idx] = []\n",
        "                        additional_answers[idx].append(ex['input_text'])\n",
        "        for i in range(len(datum['questions'])):\n",
        "            question, answer = datum['questions'][i], datum['answers'][i]\n",
        "            assert question['turn_id'] == answer['turn_id']\n",
        "            idx = question['turn_id']\n",
        "            _qas = {\n",
        "                'turn_id': idx,\n",
        "                'question': question['input_text'],\n",
        "                'answer': answer['input_text']\n",
        "            }\n",
        "            if idx in additional_answers:\n",
        "                _qas['additional_answers'] = additional_answers[idx]\n",
        "            _qas['raw_answer'] = answer['input_text']\n",
        "            if _qas['raw_answer'].lower() in ['yes', 'yes.']:\n",
        "                _qas['raw_answer'] = 'yes'\n",
        "            if _qas['raw_answer'].lower() in ['no', 'no.']:\n",
        "                _qas['raw_answer'] = 'no'\n",
        "            if _qas['raw_answer'].lower() in ['unknown', 'unknown.']:\n",
        "                _qas['raw_answer'] = 'unknown'\n",
        "            _qas['answer_span_start'] = answer['span_start']\n",
        "            _qas['answer_span_end'] = answer['span_end']\n",
        "            start = answer['span_start']\n",
        "            end = answer['span_end']\n",
        "            chosen_text = _datum['context'][start:end].lower()\n",
        "            while len(chosen_text) > 0 and self.is_whitespace(chosen_text[0]):\n",
        "                chosen_text = chosen_text[1:]\n",
        "                start += 1\n",
        "            while len(chosen_text) > 0 and self.is_whitespace(chosen_text[-1]):\n",
        "                chosen_text = chosen_text[:-1]\n",
        "                end -= 1\n",
        "            r_start, r_end = self.find_span(_datum['raw_context_offsets'], start, end)\n",
        "            input_text = _qas['answer'].strip().lower()\n",
        "            if input_text in chosen_text:\n",
        "                p = chosen_text.find(input_text)\n",
        "                _qas['answer_span'] = self.find_span(_datum['raw_context_offsets'], start + p, start + p + len(input_text))\n",
        "            else:\n",
        "                _qas['answer_span'] = self.find_span_with_gt(_datum['context'], _datum['raw_context_offsets'], input_text)\n",
        "            long_questions = []\n",
        "            for j in range(i - history_len, i + 1):\n",
        "                long_question = ''\n",
        "                if j < 0:\n",
        "                    continue\n",
        "                long_question += ' ' + datum['questions'][j]['input_text']\n",
        "                if j < i:\n",
        "                    long_question += ' ' + datum['answers'][j]['input_text'] + ' '\n",
        "                long_question = long_question.strip()\n",
        "                long_questions.append(long_question)\n",
        "            doc_tok = _datum['annotated_context']['word']\n",
        "            if len(doc_tok) == 0:\n",
        "                continue\n",
        "            example = CoqaExample(\n",
        "                qas_id = _datum['id'] + ' ' + str(_qas['turn_id']),\n",
        "                question_text = long_questions,\n",
        "                doc_tokens = doc_tok,\n",
        "                orig_answer_text = _qas['raw_answer'],\n",
        "                start_position = _qas['answer_span'][0],\n",
        "                end_position = _qas['answer_span'][1],\n",
        "                rational_start_position = r_start,\n",
        "                rational_end_position = r_end,\n",
        "                additional_answers=_qas['additional_answers'] if 'additional_answers' in _qas else None,\n",
        "            )\n",
        "            examples.append(example)\n",
        "        return examples\n",
        "\n",
        "train_examples = Processor().get_examples(evaluate = False)\n",
        "test_examples = Processor().get_examples(evaluate = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyjuYeEjshZC",
        "outputId": "5c4ed47f-9426-4727-d20e-2b5261ae5d0b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing examples: 100%|██████████| 7199/7199 [18:43<00:00,  6.41it/s]\n",
            "Preprocessing examples: 100%|██████████| 500/500 [01:18<00:00,  6.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the processed examples\n",
        "\n",
        "Now that our example questions have been processed we can explore what we have achieved.\n",
        "\n",
        "Let us take an example from the dataset -\n",
        "\n",
        "```\n",
        "Story : Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton.\n",
        "Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept.\n",
        "But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters.\n",
        "All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch.\n",
        "The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy.\n",
        "Being different made Cotton quite sad. She often wished she looked like the rest of her family.\n",
        "So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them.\n",
        "When her mommy and sisters found her they started laughing.\n",
        "\n",
        "\"What are you doing, Cotton?!\"\n",
        "\n",
        "\"I only wanted to be more like you\".\n",
        "\n",
        "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry.\n",
        "\n",
        "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\"\n",
        "\n",
        "Then Cotton thought, \"I change my mind. I like being special\"\n",
        "\n",
        "Question : What color was Cotton?\n",
        "\n",
        "Answer : {SPAN - (59 -> 93),\n",
        "          RATIONALE - a little white kitten named Cotton,\n",
        "          GROUND TRUTH - white }\n",
        "\n",
        "Question : Did she live alone?\n",
        "\n",
        "Answer : {SPAN - (196 -> 215),\n",
        "          RATIONALE - Cotton wasn't alone,\n",
        "          GROUND TRUTH - no, }\n",
        "\n",
        "```\n",
        "\n",
        "**And see what our pre-processing has done to it...**\n"
      ],
      "metadata": {
        "id": "EbPm-QQD-r_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_examples[0])\n",
        "print(test_examples[0].doc_tokens[test_examples[0].start_position:test_examples[0].end_position+1])\n",
        "\n",
        "print(test_examples[2])\n",
        "print(test_examples[2].doc_tokens[test_examples[2].start_position:test_examples[2].end_position+1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1jfSZor-nfM",
        "outputId": "b986b990-915f-4d5b-ff36-26e04416989f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question : ['What color was Cotton?']\n",
            "                   Document : ['Once', 'upon', 'a', 'time', ',', 'in', 'a', 'barn', 'near', 'a', 'farm', 'house', ',', 'there', 'lived', 'a', 'little', 'white', 'kitten', 'named', 'Cotton', '.', 'Cotton', 'lived', 'high', 'up', 'in', 'a', 'nice', 'warm', 'place', 'above', 'the', 'barn', 'where', 'all', 'of', 'the', 'farmer', \"'s\", 'horses', 'slept', '.', 'But', 'Cotton', 'was', \"n't\", 'alone', 'in', 'her', 'little', 'home', 'above', 'the', 'barn', ',', 'oh', 'no', '.', 'She', 'shared', 'her', 'hay', 'bed', 'with', 'her', 'mommy', 'and', '5', 'other', 'sisters', '.', 'All', 'of', 'her', 'sisters', 'were', 'cute', 'and', 'fluffy', ',', 'like', 'Cotton', '.', 'But', 'she', 'was', 'the', 'only', 'white', 'one', 'in', 'the', 'bunch', '.', 'The', 'rest', 'of', 'her', 'sisters', 'were', 'all', 'orange', 'with', 'beautiful', 'white', 'tiger', 'stripes', 'like', 'Cotton', \"'s\", 'mommy', '.', 'Being', 'different', 'made', 'Cotton', 'quite', 'sad', '.', 'She', 'often', 'wished', 'she', 'looked', 'like', 'the', 'rest', 'of', 'her', 'family', '.', 'So', 'one', 'day', ',', 'when', 'Cotton', 'found', 'a', 'can', 'of', 'the', 'old', 'farmer', \"'s\", 'orange', 'paint', ',', 'she', 'used', 'it', 'to', 'paint', 'herself', 'like', 'them', '.', 'When', 'her', 'mommy', 'and', 'sisters', 'found', 'her', 'they', 'started', 'laughing', '.', '\"', 'What', 'are', 'you', 'doing', ',', 'Cotton', '?', '!', '\"', '\"', 'I', 'only', 'wanted', 'to', 'be', 'more', 'like', 'you', '\"', '.', 'Cotton', \"'s\", 'mommy', 'rubbed', 'her', 'face', 'on', 'Cotton', \"'s\", 'and', 'said', '\"', 'Oh', 'Cotton', ',', 'but', 'your', 'fur', 'is', 'so', 'pretty', 'and', 'special', ',', 'like', 'you', '.', 'We', 'would', 'never', 'want', 'you', 'to', 'be', 'any', 'other', 'way', '\"', '.', 'And', 'with', 'that', ',', 'Cotton', \"'s\", 'mommy', 'picked', 'her', 'up', 'and', 'dropped', 'her', 'into', 'a', 'big', 'bucket', 'of', 'water', '.', 'When', 'Cotton', 'came', 'out', 'she', 'was', 'herself', 'again', '.', 'Her', 'sisters', 'licked', 'her', 'face', 'until', 'Cotton', \"'s\", 'fur', 'was', 'all', 'all', 'dry', '.', '\"', 'Do', \"n't\", 'ever', 'do', 'that', 'again', ',', 'Cotton', '!', '\"', 'they', 'all', 'cried', '.', '\"', 'Next', 'time', 'you', 'might', 'mess', 'up', 'that', 'pretty', 'white', 'fur', 'of', 'yours', 'and', 'we', 'would', \"n't\", 'want', 'that', '!', '\"', 'Then', 'Cotton', 'thought', ',', '\"', 'I', 'change', 'my', 'mind', '.', 'I', 'like', 'being', 'special', '\"', '.']\n",
            "                   Ground Truth : white\n",
            "                   Answer Span : 17 -> 17\n",
            "['white']\n",
            "Question : ['What color was Cotton? white', 'Where did she live? in a barn', 'Did she live alone?']\n",
            "                   Document : ['Once', 'upon', 'a', 'time', ',', 'in', 'a', 'barn', 'near', 'a', 'farm', 'house', ',', 'there', 'lived', 'a', 'little', 'white', 'kitten', 'named', 'Cotton', '.', 'Cotton', 'lived', 'high', 'up', 'in', 'a', 'nice', 'warm', 'place', 'above', 'the', 'barn', 'where', 'all', 'of', 'the', 'farmer', \"'s\", 'horses', 'slept', '.', 'But', 'Cotton', 'was', \"n't\", 'alone', 'in', 'her', 'little', 'home', 'above', 'the', 'barn', ',', 'oh', 'no', '.', 'She', 'shared', 'her', 'hay', 'bed', 'with', 'her', 'mommy', 'and', '5', 'other', 'sisters', '.', 'All', 'of', 'her', 'sisters', 'were', 'cute', 'and', 'fluffy', ',', 'like', 'Cotton', '.', 'But', 'she', 'was', 'the', 'only', 'white', 'one', 'in', 'the', 'bunch', '.', 'The', 'rest', 'of', 'her', 'sisters', 'were', 'all', 'orange', 'with', 'beautiful', 'white', 'tiger', 'stripes', 'like', 'Cotton', \"'s\", 'mommy', '.', 'Being', 'different', 'made', 'Cotton', 'quite', 'sad', '.', 'She', 'often', 'wished', 'she', 'looked', 'like', 'the', 'rest', 'of', 'her', 'family', '.', 'So', 'one', 'day', ',', 'when', 'Cotton', 'found', 'a', 'can', 'of', 'the', 'old', 'farmer', \"'s\", 'orange', 'paint', ',', 'she', 'used', 'it', 'to', 'paint', 'herself', 'like', 'them', '.', 'When', 'her', 'mommy', 'and', 'sisters', 'found', 'her', 'they', 'started', 'laughing', '.', '\"', 'What', 'are', 'you', 'doing', ',', 'Cotton', '?', '!', '\"', '\"', 'I', 'only', 'wanted', 'to', 'be', 'more', 'like', 'you', '\"', '.', 'Cotton', \"'s\", 'mommy', 'rubbed', 'her', 'face', 'on', 'Cotton', \"'s\", 'and', 'said', '\"', 'Oh', 'Cotton', ',', 'but', 'your', 'fur', 'is', 'so', 'pretty', 'and', 'special', ',', 'like', 'you', '.', 'We', 'would', 'never', 'want', 'you', 'to', 'be', 'any', 'other', 'way', '\"', '.', 'And', 'with', 'that', ',', 'Cotton', \"'s\", 'mommy', 'picked', 'her', 'up', 'and', 'dropped', 'her', 'into', 'a', 'big', 'bucket', 'of', 'water', '.', 'When', 'Cotton', 'came', 'out', 'she', 'was', 'herself', 'again', '.', 'Her', 'sisters', 'licked', 'her', 'face', 'until', 'Cotton', \"'s\", 'fur', 'was', 'all', 'all', 'dry', '.', '\"', 'Do', \"n't\", 'ever', 'do', 'that', 'again', ',', 'Cotton', '!', '\"', 'they', 'all', 'cried', '.', '\"', 'Next', 'time', 'you', 'might', 'mess', 'up', 'that', 'pretty', 'white', 'fur', 'of', 'yours', 'and', 'we', 'would', \"n't\", 'want', 'that', '!', '\"', 'Then', 'Cotton', 'thought', ',', '\"', 'I', 'change', 'my', 'mind', '.', 'I', 'like', 'being', 'special', '\"', '.']\n",
            "                   Ground Truth : no\n",
            "                   Answer Span : 57 -> 57\n",
            "['no']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the document **has been turned into an array of words** and the ground truth answer has been turned into **spans** from the document.\n",
        "\n",
        "Play around with other examples from the dataset to see this in action.\n",
        "\n",
        "# Now that we have the examples processed we can convert them to vectors\n"
      ],
      "metadata": {
        "id": "S0_T1k4W4kK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text):\n",
        "    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
        "    for new_start in range(input_start, input_end + 1):\n",
        "        for new_end in range(input_end, new_start - 1, -1):\n",
        "            text_span = \" \".join(doc_tokens[new_start: (new_end + 1)])\n",
        "            if text_span == tok_answer_text:\n",
        "                return (new_start, new_end)\n",
        "    return (input_start, input_end)\n",
        "\n",
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "    best_score = None\n",
        "    best_span_index = None\n",
        "    for (span_index, doc_span) in enumerate(doc_spans):\n",
        "        end = doc_span.start + doc_span.length - 1\n",
        "        if position < doc_span.start:\n",
        "            continue\n",
        "        if position > end:\n",
        "            continue\n",
        "        num_left_context = position - doc_span.start\n",
        "        num_right_context = end - position\n",
        "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_span_index = span_index\n",
        "    return cur_span_index == best_span_index\n",
        "\n",
        "def Extract_Feature_init(tokenizer_for_convert):\n",
        "    global tokenizer\n",
        "    tokenizer = tokenizer_for_convert\n",
        "\n",
        "def Extract_Feature(example, tokenizer, max_seq_length = 512, doc_stride = 128, max_query_length = 64):\n",
        "    \"\"\" Extract features for a single turn of QA \"\"\"\n",
        "    features = []\n",
        "    query_tokens = []\n",
        "    for question_answer in example.question_text:\n",
        "        query_tokens.extend(tokenizer.tokenize(question_answer))\n",
        "    cls_idx = 3\n",
        "    if example.orig_answer_text == 'yes':\n",
        "        cls_idx = 0  # yes\n",
        "    elif example.orig_answer_text == 'no':\n",
        "        cls_idx = 1  # no\n",
        "    elif example.orig_answer_text == 'unknown':\n",
        "        cls_idx = 2  # unknown\n",
        "    if len(query_tokens) > max_query_length:\n",
        "        # keep tail\n",
        "        query_tokens = query_tokens[-max_query_length:]\n",
        "    tok_to_orig_index = []\n",
        "    orig_to_tok_index = []\n",
        "    all_doc_tokens = []\n",
        "    for (i, token) in enumerate(example.doc_tokens):\n",
        "        orig_to_tok_index.append(len(all_doc_tokens))\n",
        "        sub_tokens = tokenizer.tokenize(token)\n",
        "        for sub_token in sub_tokens:\n",
        "            tok_to_orig_index.append(i)\n",
        "            all_doc_tokens.append(sub_token)\n",
        "    tok_r_start_position = orig_to_tok_index[example.rational_start_position]\n",
        "    if example.rational_end_position < len(example.doc_tokens) - 1:\n",
        "        tok_r_end_position = orig_to_tok_index[example.rational_end_position + 1] - 1\n",
        "    else:\n",
        "        tok_r_end_position = len(all_doc_tokens) - 1\n",
        "    if cls_idx < 3:\n",
        "        tok_start_position, tok_end_position = 0, 0\n",
        "    else:\n",
        "        tok_start_position = orig_to_tok_index[example.start_position]\n",
        "        if example.end_position < len(example.doc_tokens) - 1:\n",
        "            tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
        "        else:\n",
        "            tok_end_position = len(all_doc_tokens) - 1\n",
        "        (tok_start_position, tok_end_position) = _improve_answer_span(\n",
        "            all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
        "            example.orig_answer_text)\n",
        "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
        "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "    _DocSpan = collections.namedtuple(\"DocSpan\", [\"start\", \"length\"])\n",
        "    doc_spans = []\n",
        "    start_offset = 0\n",
        "    while start_offset < len(all_doc_tokens):\n",
        "        length = len(all_doc_tokens) - start_offset\n",
        "        if length > max_tokens_for_doc:\n",
        "            length = max_tokens_for_doc\n",
        "        doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "        if start_offset + length == len(all_doc_tokens):\n",
        "            break\n",
        "        start_offset += min(length, doc_stride)\n",
        "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "        slice_cls_idx = cls_idx\n",
        "        tokens = []\n",
        "        token_to_orig_map = {}\n",
        "        token_is_max_context = {}\n",
        "        segment_ids = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        segment_ids.append(0)\n",
        "        for token in query_tokens:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(0)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(0)\n",
        "\n",
        "        for i in range(doc_span.length):\n",
        "            split_token_index = doc_span.start + i\n",
        "            token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
        "\n",
        "            is_max_context = _check_is_max_context(doc_spans,\n",
        "                                                   doc_span_index,\n",
        "                                                   split_token_index)\n",
        "            token_is_max_context[len(tokens)] = is_max_context\n",
        "            tokens.append(all_doc_tokens[split_token_index])\n",
        "            segment_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(1)\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "            segment_ids.append(0)\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "        doc_start = doc_span.start\n",
        "        doc_end = doc_span.start + doc_span.length - 1\n",
        "        out_of_span = False\n",
        "        if example.rational_start_position == -1 or not (\n",
        "                tok_r_start_position >= doc_start and tok_r_end_position <= doc_end):\n",
        "            out_of_span = True\n",
        "        if out_of_span:\n",
        "            rational_start_position = 0\n",
        "            rational_end_position = 0\n",
        "        else:\n",
        "            doc_offset = len(query_tokens) + 2\n",
        "            rational_start_position = tok_r_start_position - doc_start + doc_offset\n",
        "            rational_end_position = tok_r_end_position - doc_start + doc_offset\n",
        "\n",
        "        rational_mask = [0] * len(input_ids)\n",
        "        if not out_of_span:\n",
        "            rational_mask[rational_start_position:rational_end_position + 1] = [1] * (\n",
        "                        rational_end_position - rational_start_position + 1)\n",
        "\n",
        "        if cls_idx >= 3:\n",
        "            # For training, if our document chunk does not contain an annotation we remove it\n",
        "            doc_start = doc_span.start\n",
        "            doc_end = doc_span.start + doc_span.length - 1\n",
        "            out_of_span = False\n",
        "            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):\n",
        "                out_of_span = True\n",
        "            if out_of_span:\n",
        "                start_position = 0\n",
        "                end_position = 0\n",
        "                slice_cls_idx = 2\n",
        "            else:\n",
        "                doc_offset = len(query_tokens) + 2\n",
        "                start_position = tok_start_position - doc_start + doc_offset\n",
        "                end_position = tok_end_position - doc_start + doc_offset\n",
        "        else:\n",
        "            start_position = 0\n",
        "            end_position = 0\n",
        "\n",
        "        features.append(\n",
        "            CoqaFeatures(example_index=0,\n",
        "                         unique_id=0,\n",
        "                         doc_span_index=doc_span_index,\n",
        "                         tokens=tokens,\n",
        "                         token_to_orig_map=token_to_orig_map,\n",
        "                         token_is_max_context=token_is_max_context,\n",
        "                         input_ids=input_ids,\n",
        "                         input_mask=input_mask,\n",
        "                         segment_ids=segment_ids,\n",
        "                         start_position=start_position,\n",
        "                         end_position=end_position,\n",
        "                         cls_idx=slice_cls_idx,\n",
        "                         rational_mask=rational_mask))\n",
        "    return features\n",
        "\n",
        "\n",
        "def Extract_Features(examples, tokenizer, max_seq_length, doc_stride, max_query_length, is_training,threads=1):\n",
        "    \"\"\" Convert to vectors \"\"\"\n",
        "    features = []\n",
        "    threads = min(threads, cpu_count())\n",
        "    with Pool(threads, initializer=Extract_Feature_init, initargs=(tokenizer,)) as p:\n",
        "        annotate_ = partial( Extract_Feature, tokenizer=tokenizer, max_seq_length=max_seq_length,\n",
        "                doc_stride=doc_stride, max_query_length=max_query_length,)\n",
        "        features = list( tqdm( p.imap(annotate_, examples, chunksize=32),\n",
        "                              total=len(examples), desc=\"Extracting features from dataset\",))\n",
        "    new_features = []\n",
        "    unique_id = 1000000000\n",
        "    example_index = 0\n",
        "    for example_features in tqdm(features, total=len(features), desc=\"Tag unique id to each example\"):\n",
        "        if not example_features:\n",
        "            continue\n",
        "        for example_feature in example_features:\n",
        "            example_feature.example_index = example_index\n",
        "            example_feature.unique_id = unique_id\n",
        "            new_features.append(example_feature)\n",
        "            unique_id += 1\n",
        "        example_index += 1\n",
        "    features = new_features\n",
        "    del new_features\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_tokentype_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    if not is_training:\n",
        "        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids, all_tokentype_ids, all_input_mask, all_example_index)\n",
        "    else:\n",
        "        all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n",
        "        all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n",
        "        all_rational_mask = torch.tensor([f.rational_mask for f in features], dtype=torch.long)\n",
        "        all_cls_idx = torch.tensor([f.cls_idx for f in features], dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids, all_tokentype_ids, all_input_mask, all_start_positions,\n",
        "                                all_end_positions, all_rational_mask, all_cls_idx)\n",
        "    return features, dataset"
      ],
      "metadata": {
        "id": "0tzG0Uil4kgc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def load_dataset(examples, tokenizer, evaluate=False):\n",
        "    features, dataset = Extract_Features(examples=examples, tokenizer=tokenizer,\n",
        "                    max_seq_length=512, doc_stride=128, max_query_length=64, is_training=not evaluate, threads=12)\n",
        "    return dataset, features\n",
        "\n",
        "train_dataset, train_features = load_dataset(train_examples, tokenizer)\n",
        "test_dataset, test_features = load_dataset(test_examples, tokenizer, evaluate = True)"
      ],
      "metadata": {
        "id": "xqoeFjXRAIeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ef0b2c-436f-4409-e469-623bf6822ae5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features from dataset: 100%|██████████| 108647/108647 [09:43<00:00, 186.25it/s]\n",
            "Tag unique id to each example: 100%|██████████| 108647/108647 [00:00<00:00, 779599.55it/s]\n",
            "Extracting features from dataset: 100%|██████████| 7983/7983 [00:43<00:00, 183.47it/s]\n",
            "Tag unique id to each example: 100%|██████████| 7983/7983 [00:00<00:00, 661604.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let us explore the extracted features with a few examples\n",
        "\n",
        "Note how we transform the input with addition of special tokens and all our pre-processing.\n",
        "\n",
        "What started as a phrase from the context paragraph, now becomes a span of tokens."
      ],
      "metadata": {
        "id": "TbjN47nfbvX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ex = test_features[0]\n",
        "\n",
        "print(ex.tokens)\n",
        "print(f\"Answer : {ex.tokens[ex.start_position:ex.end_position+1]} which is position ({ex.start_position}, {ex.end_position})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKjHUZV0b-Xv",
        "outputId": "61267c7d-fdc1-438d-ebe3-995f8708dc1c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'what', 'color', 'was', 'cotton', '?', '[SEP]', 'once', 'upon', 'a', 'time', ',', 'in', 'a', 'barn', 'near', 'a', 'farm', 'house', ',', 'there', 'lived', 'a', 'little', 'white', 'kitten', 'named', 'cotton', '.', 'cotton', 'lived', 'high', 'up', 'in', 'a', 'nice', 'warm', 'place', 'above', 'the', 'barn', 'where', 'all', 'of', 'the', 'farmer', \"'\", 's', 'horses', 'slept', '.', 'but', 'cotton', 'was', 'n', \"'\", 't', 'alone', 'in', 'her', 'little', 'home', 'above', 'the', 'barn', ',', 'oh', 'no', '.', 'she', 'shared', 'her', 'hay', 'bed', 'with', 'her', 'mommy', 'and', '5', 'other', 'sisters', '.', 'all', 'of', 'her', 'sisters', 'were', 'cute', 'and', 'fluffy', ',', 'like', 'cotton', '.', 'but', 'she', 'was', 'the', 'only', 'white', 'one', 'in', 'the', 'bunch', '.', 'the', 'rest', 'of', 'her', 'sisters', 'were', 'all', 'orange', 'with', 'beautiful', 'white', 'tiger', 'stripes', 'like', 'cotton', \"'\", 's', 'mommy', '.', 'being', 'different', 'made', 'cotton', 'quite', 'sad', '.', 'she', 'often', 'wished', 'she', 'looked', 'like', 'the', 'rest', 'of', 'her', 'family', '.', 'so', 'one', 'day', ',', 'when', 'cotton', 'found', 'a', 'can', 'of', 'the', 'old', 'farmer', \"'\", 's', 'orange', 'paint', ',', 'she', 'used', 'it', 'to', 'paint', 'herself', 'like', 'them', '.', 'when', 'her', 'mommy', 'and', 'sisters', 'found', 'her', 'they', 'started', 'laughing', '.', '\"', 'what', 'are', 'you', 'doing', ',', 'cotton', '?', '!', '\"', '\"', 'i', 'only', 'wanted', 'to', 'be', 'more', 'like', 'you', '\"', '.', 'cotton', \"'\", 's', 'mommy', 'rubbed', 'her', 'face', 'on', 'cotton', \"'\", 's', 'and', 'said', '\"', 'oh', 'cotton', ',', 'but', 'your', 'fur', 'is', 'so', 'pretty', 'and', 'special', ',', 'like', 'you', '.', 'we', 'would', 'never', 'want', 'you', 'to', 'be', 'any', 'other', 'way', '\"', '.', 'and', 'with', 'that', ',', 'cotton', \"'\", 's', 'mommy', 'picked', 'her', 'up', 'and', 'dropped', 'her', 'into', 'a', 'big', 'bucket', 'of', 'water', '.', 'when', 'cotton', 'came', 'out', 'she', 'was', 'herself', 'again', '.', 'her', 'sisters', 'licked', 'her', 'face', 'until', 'cotton', \"'\", 's', 'fur', 'was', 'all', 'all', 'dry', '.', '\"', 'do', 'n', \"'\", 't', 'ever', 'do', 'that', 'again', ',', 'cotton', '!', '\"', 'they', 'all', 'cried', '.', '\"', 'next', 'time', 'you', 'might', 'mess', 'up', 'that', 'pretty', 'white', 'fur', 'of', 'yours', 'and', 'we', 'would', 'n', \"'\", 't', 'want', 'that', '!', '\"', 'then', 'cotton', 'thought', ',', '\"', 'i', 'change', 'my', 'mind', '.', 'i', 'like', 'being', 'special', '\"', '.', '[SEP]']\n",
            "Answer : ['white'] which is position (24, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, the transformer needs \"one-hot-embeddings\", or the index of each token. The input to the transformer looks like the following. Where [CLS] has token id 101, \"what\" has token id 2054, etc"
      ],
      "metadata": {
        "id": "JvkXDwSndWcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ex.input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ6DCuT8dkx1",
        "outputId": "6d5a1ea1-9051-40db-e64e-5d6d3db22e00"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2054, 3609, 2001, 6557, 1029, 102, 2320, 2588, 1037, 2051, 1010, 1999, 1037, 8659, 2379, 1037, 3888, 2160, 1010, 2045, 2973, 1037, 2210, 2317, 18401, 2315, 6557, 1012, 6557, 2973, 2152, 2039, 1999, 1037, 3835, 4010, 2173, 2682, 1996, 8659, 2073, 2035, 1997, 1996, 7500, 1005, 1055, 5194, 7771, 1012, 2021, 6557, 2001, 1050, 1005, 1056, 2894, 1999, 2014, 2210, 2188, 2682, 1996, 8659, 1010, 2821, 2053, 1012, 2016, 4207, 2014, 10974, 2793, 2007, 2014, 20565, 1998, 1019, 2060, 5208, 1012, 2035, 1997, 2014, 5208, 2020, 10140, 1998, 27036, 1010, 2066, 6557, 1012, 2021, 2016, 2001, 1996, 2069, 2317, 2028, 1999, 1996, 9129, 1012, 1996, 2717, 1997, 2014, 5208, 2020, 2035, 4589, 2007, 3376, 2317, 6816, 13560, 2066, 6557, 1005, 1055, 20565, 1012, 2108, 2367, 2081, 6557, 3243, 6517, 1012, 2016, 2411, 6257, 2016, 2246, 2066, 1996, 2717, 1997, 2014, 2155, 1012, 2061, 2028, 2154, 1010, 2043, 6557, 2179, 1037, 2064, 1997, 1996, 2214, 7500, 1005, 1055, 4589, 6773, 1010, 2016, 2109, 2009, 2000, 6773, 2841, 2066, 2068, 1012, 2043, 2014, 20565, 1998, 5208, 2179, 2014, 2027, 2318, 5870, 1012, 1000, 2054, 2024, 2017, 2725, 1010, 6557, 1029, 999, 1000, 1000, 1045, 2069, 2359, 2000, 2022, 2062, 2066, 2017, 1000, 1012, 6557, 1005, 1055, 20565, 7503, 2014, 2227, 2006, 6557, 1005, 1055, 1998, 2056, 1000, 2821, 6557, 1010, 2021, 2115, 6519, 2003, 2061, 3492, 1998, 2569, 1010, 2066, 2017, 1012, 2057, 2052, 2196, 2215, 2017, 2000, 2022, 2151, 2060, 2126, 1000, 1012, 1998, 2007, 2008, 1010, 6557, 1005, 1055, 20565, 3856, 2014, 2039, 1998, 3333, 2014, 2046, 1037, 2502, 13610, 1997, 2300, 1012, 2043, 6557, 2234, 2041, 2016, 2001, 2841, 2153, 1012, 2014, 5208, 11181, 2014, 2227, 2127, 6557, 1005, 1055, 6519, 2001, 2035, 2035, 4318, 1012, 1000, 2079, 1050, 1005, 1056, 2412, 2079, 2008, 2153, 1010, 6557, 999, 1000, 2027, 2035, 6639, 1012, 1000, 2279, 2051, 2017, 2453, 6752, 2039, 2008, 3492, 2317, 6519, 1997, 6737, 1998, 2057, 2052, 1050, 1005, 1056, 2215, 2008, 999, 1000, 2059, 6557, 2245, 1010, 1000, 1045, 2689, 2026, 2568, 1012, 1045, 2066, 2108, 2569, 1000, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we can define our model and train it.\n",
        "\n",
        "In the following code blocks we will first define our transformer model, followed by training."
      ],
      "metadata": {
        "id": "ce26W1-dd3za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertModel\n",
        "\n",
        "\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.span_modelling = nn.Linear(hidden_size, 2,bias = False)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size,hidden_size, bias = False)\n",
        "        self.fc2 = nn.Linear(hidden_size,hidden_size, bias = False)\n",
        "        self.rationale_modelling = nn.Linear(hidden_size, 1, bias = False)\n",
        "        self.attention_modelling = nn.Linear(hidden_size,1, bias = False)\n",
        "        self.unk_modelling = nn.Linear(2*hidden_size,1, bias = False)\n",
        "        self.yes_no_modelling = nn.Linear(2*hidden_size,2, bias = False)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.beta = 5.0\n",
        "    def forward(self, input_ids, segment_ids=None, input_masks=None,\n",
        "                start_positions=None, end_positions=None, rationale_mask=None, cls_idx=None):\n",
        "        #   Bert-base outputs\n",
        "        output_vector, bert_pooled_output = self.bert(input_ids,token_type_ids=segment_ids,attention_mask=input_masks,\n",
        "                                                      head_mask = None, return_dict=False) # output from BERT model\n",
        "        start_end_logits = self.span_modelling(output_vector)  #predict start and end positions\n",
        "        start_logits, end_logits = start_end_logits.split(1, dim=-1)\n",
        "        start_logits, end_logits = start_logits.squeeze(-1), end_logits.squeeze(-1)\n",
        "\n",
        "        rationale_logits = self.relu(self.fc(output_vector))\n",
        "        rationale_logits = self.rationale_modelling(rationale_logits)\n",
        "        rationale_logits = torch.sigmoid(rationale_logits) #predict rationale positions\n",
        "\n",
        "        output_vector = output_vector * rationale_logits\n",
        "        attention  = self.relu(self.fc2(output_vector))\n",
        "        attention  = (self.attention_modelling(attention)).squeeze(-1)\n",
        "        input_masks = input_masks.type(attention.dtype)\n",
        "        attention = attention*input_masks - (1-input_masks)*1e30\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention_pooled_output = (attention.unsqueeze(-1) * output_vector).sum(dim=-2)\n",
        "        cls_output = torch.cat((attention_pooled_output,bert_pooled_output),dim = -1) #predict yes/no/unk\n",
        "        rationale_logits = rationale_logits.squeeze(-1)\n",
        "        unk_logits = self.unk_modelling(cls_output)\n",
        "        yes_no_logits = self.yes_no_modelling(cls_output)\n",
        "        yes_logits, no_logits = yes_no_logits.split(1, dim=-1)\n",
        "        if self.training:\n",
        "            start_positions, end_positions = start_positions + cls_idx, end_positions + cls_idx #predict start and end positions\n",
        "            start = torch.cat((yes_logits, no_logits, unk_logits, start_logits), dim=-1)\n",
        "            end = torch.cat((yes_logits, no_logits, unk_logits, end_logits), dim=-1)\n",
        "            Entropy_loss = CrossEntropyLoss()\n",
        "            start_loss = Entropy_loss(start, start_positions)\n",
        "            end_loss = Entropy_loss(end, end_positions)\n",
        "            rationale_positions = rationale_mask.type(attention.dtype) #predict rationale positions\n",
        "            rationale_loss = (-rationale_positions*torch.log(rationale_logits + 1e-8)\n",
        "                             - (1-rationale_positions)*torch.log(1-rationale_logits + 1e-8))\n",
        "            rationale_loss = torch.mean(rationale_loss)\n",
        "            total_loss = (start_loss + end_loss) / 2.0 + rationale_loss * self.beta #calculate total loss (multi-task)\n",
        "            return total_loss\n",
        "        return start_logits, end_logits, yes_logits, no_logits, unk_logits\n",
        "\n",
        "model = Bert()"
      ],
      "metadata": {
        "id": "o__gVCNueDsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "!mkdir \"./weights\"\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "\n",
        "def train(model):\n",
        "    epochs = 1\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle = True)\n",
        "    optimizer_parameters = [{\"params\": [p for n, p in model.named_parameters()  #Which parameters to optimize\n",
        "                                if not any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])],\"weight_decay\": 0.01,},\n",
        "                            {\"params\": [p for n, p in model.named_parameters()\n",
        "                                if any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])], \"weight_decay\": 0.0}]\n",
        "    optimizer = AdamW(optimizer_parameters, lr = 3e-5, eps = 1e-8) #The optimizer (AdamW)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps=500, num_training_steps=(len(train_dataloader)//epochs))\n",
        "    # Learning rate scheduler\n",
        "    counter,train_loss, loss = 1, 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    for ep in range(epochs): #Epoch iteration\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "        for i,batch in enumerate(epoch_iterator): #Iterating over mini-batches\n",
        "            model.train()\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = { \"input_ids\": batch[0],\"segment_ids\": batch[1],\n",
        "                  \"input_masks\": batch[2],\"start_positions\": batch[3],\n",
        "                  \"end_positions\": batch[4],\"rationale_mask\": batch[5],\"cls_idx\": batch[6]}\n",
        "            loss = model(**inputs)\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            model.zero_grad()\n",
        "            counter += 1\n",
        "            epoch_iterator.set_description(f\"Epoch {ep+1}/{epochs} | Loss : {(train_loss/counter)}\")\n",
        "            epoch_iterator.refresh()\n",
        "            if counter % 1000 == 0:\n",
        "                save_path = os.path.join(\"./weights\", \"weights.pth\")\n",
        "                torch.save(model.state_dict(), save_path)\n",
        "    return model\n",
        "\n",
        "trained_model = train(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNKC_J7efE_K",
        "outputId": "40081a97-2418-4665-8cc9-587beb634555"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘./weights’: File exists\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1 | Loss : 2.452767640136831: 100%|██████████| 7193/7193 [55:26<00:00,  2.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we need our inference logic\n",
        "\n",
        "We need to convert the span predictions, yes/no/unk predictions, etc to a textual answer.\n",
        "\n",
        "The logic is roughly as follows.\n",
        "  - Get the best yes, no, unknown prediction scores.\n",
        "  - Get top 20 start positions and end positions (400 spans).\n",
        "  - Out of these choose top 20 again.\n",
        "  - Convert these to text (yes, no, unknown or from the span).\n",
        "  - Softmax over the scores of these candidates.\n",
        "  - Pick best answer."
      ],
      "metadata": {
        "id": "bAlvFTNEuF5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.tokenization_bert import BasicTokenizer\n",
        "import math\n",
        "\n",
        "def get_predictions(all_examples, all_features, all_results, n_best_size, max_answer_length, do_lower_case, output_prediction_file, verbose_logging, tokenizer):\n",
        "    example_index_to_features = collections.defaultdict(list)\n",
        "    for feature in all_features:\n",
        "        example_index_to_features[feature.example_index].append(feature)\n",
        "    unique_id_to_result = {}\n",
        "    for result in all_results:\n",
        "        unique_id_to_result[result.unique_id] = result\n",
        "    _PrelimPrediction = collections.namedtuple(\"PrelimPrediction\", [\"feature_index\", \"start_index\", \"end_index\", \"score\", \"cls_idx\",])\n",
        "    all_predictions = []\n",
        "    all_nbest_json = collections.OrderedDict()\n",
        "    for (example_index, example) in enumerate(tqdm(all_examples, desc=\"Writing preditions\")):\n",
        "        features = example_index_to_features[example_index]\n",
        "        prelim_predictions = []\n",
        "        score_yes, score_no, score_span, score_unk = -float('INF'), -float('INF'), -float('INF'), -float('INF')\n",
        "        min_unk_feature_index, max_yes_feature_index, max_no_feature_index, max_span_feature_index = -1, -1, -1, -1\n",
        "        for (feature_index, feature) in enumerate(features):\n",
        "            result = unique_id_to_result[feature.unique_id]\n",
        "            feature_yes_score, feature_no_score, feature_unk_score = \\\n",
        "                result.yes_logits[0] * 2, result.no_logits[0] * 2, result.unk_logits[0] * 2\n",
        "            # Compute 20 best span starts/span ends\n",
        "            start_indexes, end_indexes = _get_best_indexes(result.start_logits, n_best_size), \\\n",
        "                                         _get_best_indexes(result.end_logits, n_best_size)\n",
        "            # Compute top (20x20) span scores\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    \"\"\" Perform validation \"\"\"\n",
        "                    if start_index >= len(feature.tokens):\n",
        "                        continue\n",
        "                    if end_index >= len(feature.tokens):\n",
        "                        continue\n",
        "                    if start_index not in feature.token_to_orig_map:\n",
        "                        continue\n",
        "                    if end_index not in feature.token_to_orig_map:\n",
        "                        continue\n",
        "                    if not feature.token_is_max_context.get(start_index, False):\n",
        "                        continue\n",
        "                    if end_index < start_index:\n",
        "                        continue\n",
        "                    length = end_index - start_index + 1\n",
        "                    if length > max_answer_length:\n",
        "                        continue\n",
        "                    feature_span_score = result.start_logits[start_index] + result.end_logits[end_index]\n",
        "                    prelim_predictions.append(_PrelimPrediction(feature_index=feature_index,start_index=start_index,end_index=end_index,score=feature_span_score,cls_idx=3))\n",
        "            if feature_unk_score < score_unk:\n",
        "                score_unk = feature_unk_score\n",
        "                min_unk_feature_index = feature_index\n",
        "            if feature_yes_score > score_yes:\n",
        "                score_yes = feature_yes_score\n",
        "                max_yes_feature_index = feature_index\n",
        "            if feature_no_score > score_no:\n",
        "                score_no = feature_no_score\n",
        "                max_no_feature_index = feature_index\n",
        "        #including yes/no/unknown answers in preliminary predictions.\n",
        "        prelim_predictions.append(_PrelimPrediction(feature_index=min_unk_feature_index,start_index=0,end_index=0,score=score_unk,cls_idx=2))\n",
        "        prelim_predictions.append(_PrelimPrediction(feature_index=max_yes_feature_index,start_index=0,end_index=0,score=score_yes,cls_idx=0))\n",
        "        prelim_predictions.append(_PrelimPrediction(feature_index=max_no_feature_index,start_index=0,end_index=0,score=score_no,cls_idx=1))\n",
        "        # Get best yes/no/unk/span\n",
        "        prelim_predictions = sorted(prelim_predictions,key=lambda p: p.score,reverse=True)\n",
        "        _NbestPrediction = collections.namedtuple(\"NbestPrediction\", [\"text\", \"score\", \"cls_idx\"])\n",
        "        seen_predictions = {}\n",
        "        nbest = []\n",
        "        # From all preliminary predictions choose top 20\n",
        "        for pred in prelim_predictions:\n",
        "            if len(nbest) >= n_best_size:\n",
        "                break\n",
        "            feature = features[pred.feature_index]\n",
        "            # free-form answers (ie span answers)\n",
        "            if pred.cls_idx == 3:\n",
        "                tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
        "                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "                orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
        "                tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n",
        "                # removing whitespaces\n",
        "                tok_text = tok_text.strip()\n",
        "                tok_text = \" \".join(tok_text.split())\n",
        "                orig_text = \" \".join(orig_tokens)\n",
        "                final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n",
        "                if final_text in seen_predictions:\n",
        "                    continue\n",
        "                seen_predictions[final_text] = True\n",
        "                nbest.append(_NbestPrediction(text=final_text,score=pred.score,cls_idx=pred.cls_idx))\n",
        "            # 'yes'/'no'/'unknown' answers\n",
        "            else:\n",
        "                text = ['yes', 'no', 'unknown']\n",
        "                nbest.append(_NbestPrediction(text=text[pred.cls_idx], score=pred.score, cls_idx=pred.cls_idx))\n",
        "        if len(nbest) < 1:\n",
        "            nbest.append(_NbestPrediction(text='unknown', score=-float('inf'), cls_idx=2))\n",
        "        assert len(nbest) >= 1\n",
        "        probs = _compute_softmax([p.score for p in nbest])\n",
        "        nbest_json = []\n",
        "        # Get final best predictions\n",
        "        for i, entry in enumerate(nbest):\n",
        "            output = collections.OrderedDict()\n",
        "            output[\"text\"] = entry.text\n",
        "            output[\"probability\"] = probs[i]\n",
        "            output[\"score\"] = entry.score\n",
        "            nbest_json.append(output)\n",
        "        assert len(nbest_json) >= 1\n",
        "        _id, _turn_id = example.qas_id.split()\n",
        "        all_predictions.append({\n",
        "            'id': _id,\n",
        "            'turn_id': int(_turn_id),\n",
        "            'answer': confirm_preds(nbest_json)})\n",
        "        all_nbest_json[example.qas_id] = nbest_json\n",
        "    #   Writing all the predictions in the predictions.json file in the BERT directory\n",
        "    with open(output_prediction_file, \"w\") as writer:\n",
        "        writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
        "    return all_predictions\n",
        "\n",
        "def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n",
        "    def _strip_spaces(text):\n",
        "        ns_chars = []\n",
        "        ns_to_s_map = collections.OrderedDict()\n",
        "        for (i, c) in enumerate(text):\n",
        "            if c == \" \":\n",
        "                continue\n",
        "            ns_to_s_map[len(ns_chars)] = i\n",
        "            ns_chars.append(c)\n",
        "        ns_text = \"\".join(ns_chars)\n",
        "        return (ns_text, ns_to_s_map)\n",
        "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
        "    start_position = tok_text.find(pred_text)\n",
        "    if start_position == -1:\n",
        "        return orig_text\n",
        "    end_position = start_position + len(pred_text) - 1\n",
        "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "    if len(orig_ns_text) != len(tok_ns_text):\n",
        "        return orig_text\n",
        "    tok_s_to_ns_map = {}\n",
        "    for (i, tok_index) in tok_ns_to_s_map.items():\n",
        "        tok_s_to_ns_map[tok_index] = i\n",
        "    orig_start_position = None\n",
        "    if start_position in tok_s_to_ns_map:\n",
        "        ns_start_position = tok_s_to_ns_map[start_position]\n",
        "        if ns_start_position in orig_ns_to_s_map:\n",
        "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "    if orig_start_position is None:\n",
        "        return orig_text\n",
        "    orig_end_position = None\n",
        "    if end_position in tok_s_to_ns_map:\n",
        "        ns_end_position = tok_s_to_ns_map[end_position]\n",
        "        if ns_end_position in orig_ns_to_s_map:\n",
        "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "    if orig_end_position is None:\n",
        "        return orig_text\n",
        "    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
        "    return output_text\n",
        "\n",
        "def confirm_preds(nbest_json):\n",
        "    #unsuccessful attempt at trying to predict for how many and True or false type of questions\n",
        "    subs = [ 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine','ten', 'eleven', 'twelve', 'true', 'false']\n",
        "    ori = nbest_json[0]['text']\n",
        "    if len(ori) < 2:\n",
        "        for e in nbest_json[1:]:\n",
        "            if _normalize_answer(e['text']) in subs:\n",
        "                return e['text']\n",
        "        return 'unknown'\n",
        "    return ori\n",
        "\n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "    return best_indexes\n",
        "\n",
        "\n",
        "def _compute_softmax(scores):\n",
        "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
        "    if not scores:\n",
        "        return []\n",
        "    max_score = None\n",
        "    for score in scores:\n",
        "        if max_score is None or score > max_score:\n",
        "            max_score = score\n",
        "    exp_scores = []\n",
        "    total_sum = 0.0\n",
        "    for score in scores:\n",
        "        x = math.exp(score - max_score)\n",
        "        exp_scores.append(x)\n",
        "        total_sum += x\n",
        "    probs = []\n",
        "    for score in exp_scores:\n",
        "        probs.append(score / total_sum)\n",
        "    return probs\n",
        "\n",
        "def _normalize_answer(s):\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    def lower(text):\n",
        "        return text.lower()"
      ],
      "metadata": {
        "id": "diIZmnOKtUqw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we can evaluate our trained model."
      ],
      "metadata": {
        "id": "DVJWWwrRxk8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_list(tensor):\n",
        "    \"\"\" Helper function to convert predictions to python list\"\"\"\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n",
        "def write_predictions(model):\n",
        "    #dataset, examples, features = load_dataset(tokenizer, evaluate=True)\n",
        "    evaluation_dataloader = DataLoader(test_dataset, batch_size=32, shuffle = False)\n",
        "    mod_results = []\n",
        "    for batch in tqdm(evaluation_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            inputs = {\"input_ids\": batch[0],\"segment_ids\": batch[1],\"input_masks\": batch[2]}\n",
        "            example_indices = batch[3]\n",
        "            outputs = model(**inputs)\n",
        "        for i, example_index in enumerate(example_indices):\n",
        "            eval_feature = test_features[example_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "            output = [convert_to_list(output[i]) for output in outputs]\n",
        "            start_logits, end_logits, yes_logits, no_logits, unk_logits = output\n",
        "            result = Result(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits, yes_logits=yes_logits, no_logits=no_logits, unk_logits=unk_logits)\n",
        "            mod_results.append(result)\n",
        "\n",
        "    output_prediction_file = os.path.join(\"./weights\", \"predictions.json\")\n",
        "    return get_predictions(test_examples, test_features, mod_results, 20, 30, True, output_prediction_file, False, tokenizer), mod_results\n",
        "\n",
        "final_predictions, model_predictions = write_predictions(trained_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF92VA0Gtbk_",
        "outputId": "7611ac7c-b14d-4407-a39c-2c60d32549d1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 268/268 [01:19<00:00,  3.36it/s]\n",
            "Writing preditions: 100%|██████████| 7983/7983 [00:29<00:00, 269.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What does a prediction look like?\n",
        "\n",
        "Notice how we are converting the logits into text (inference logic above). In this example the start,end span wins out over yes, no, unk."
      ],
      "metadata": {
        "id": "ZEO7BgEnzZFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model_predictions[0]\n",
        "print(len(logits.start_logits), len(logits.end_logits))\n",
        "print(f\"Start logits : {logits.start_logits}\")\n",
        "print(f\"Best start position : {np.argmax(logits.start_logits)}\")\n",
        "print(f\"End logits : {logits.end_logits}\")\n",
        "print(f\"Best end position : {np.argmax(logits.end_logits)}\")\n",
        "print(f\"Yes score : {logits.yes_logits}\")\n",
        "print(f\"No score : {logits.no_logits}\")\n",
        "print(f\"Unknown score : {logits.unk_logits}\")\n",
        "\n",
        "print(final_predictions[0]['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrXoWkXxzczK",
        "outputId": "06ff4068-43dc-450a-f144-0dde7991c69c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512 512\n",
            "Start logits : [-4.226983547210693, -5.639468193054199, -6.5320820808410645, -6.843592643737793, -6.946135520935059, -6.551481246948242, -7.345547676086426, -3.3142457008361816, -4.395535469055176, -5.318740367889404, -5.1656599044799805, -5.001951217651367, -2.6636996269226074, -3.4633851051330566, -4.686534881591797, -4.249269008636475, -4.287463188171387, -5.1372456550598145, -6.298287868499756, -6.521264553070068, -2.968651294708252, -4.071929931640625, -0.21001997590065002, 0.3773552179336548, 5.7509565353393555, 1.3557193279266357, -2.802481174468994, 0.12395046651363373, -3.8300960063934326, -0.7991817593574524, -3.9897940158843994, -4.150891304016113, -5.782223224639893, -4.312005043029785, -4.730404376983643, -4.716391086578369, -3.2292027473449707, -6.046236991882324, -4.6751298904418945, -5.898762226104736, -5.575925827026367, -5.49281120300293, -3.6707310676574707, -5.429015636444092, -5.542548179626465, -4.938834190368652, -6.9178571701049805, -6.691030025482178, -4.665841102600098, -5.93501615524292, -6.106950283050537, -4.291479110717773, -1.935856819152832, -3.426382541656494, -5.953868389129639, -6.797726631164551, -4.931733131408691, -4.7335004806518555, -4.460640907287598, -3.4548654556274414, -5.474928855895996, -6.21626615524292, -5.287876129150391, -6.17528772354126, -6.044258117675781, -7.2258219718933105, -4.641016006469727, -5.349266052246094, -5.737456321716309, -2.463467597961426, -4.609122276306152, -3.712934732437134, -4.481794357299805, -6.109496116638184, -4.587162971496582, -3.0595204830169678, -4.879343032836914, -4.8335280418396, -3.650815486907959, -5.577572345733643, -5.30850887298584, -6.206655025482178, -3.091022491455078, -5.310276031494141, -2.7095305919647217, -4.5955810546875, -3.9210102558135986, -3.767577648162842, -3.6206789016723633, -4.266300201416016, -6.440451622009277, -4.013125419616699, -1.9426014423370361, -6.287264823913574, -2.9328458309173584, -1.0293214321136475, -3.009732246398926, -3.482710838317871, -1.9219602346420288, 2.1036667823791504, -4.025753021240234, -4.324774742126465, -5.463652610778809, -5.682105541229248, -5.614623069763184, -4.3258466720581055, -4.561245918273926, -5.3736443519592285, -2.4766039848327637, -4.223326683044434, -1.9356435537338257, -1.097683310508728, 2.141028881072998, -2.5990898609161377, -0.9894826412200928, 0.450105756521225, -1.339794635772705, -2.8744382858276367, -4.5277419090271, -3.2353196144104004, -6.766576290130615, -6.181195259094238, -4.414484977722168, -6.053718566894531, -5.0261735916137695, -4.076285362243652, -5.1075968742370605, -2.90934419631958, -3.925989866256714, -5.196020126342773, -6.5330729484558105, -3.424818515777588, -5.99951696395874, -6.152701377868652, -3.6696434020996094, -5.036368370056152, -5.274321556091309, -5.9098076820373535, -5.680307388305664, -6.717916488647461, -4.285673141479492, -5.973377704620361, -6.719071388244629, -5.499114036560059, -4.84604549407959, -7.1220173835754395, -7.281628608703613, -5.824862003326416, -3.4315438270568848, -5.198165416717529, -5.031798362731934, -5.37648344039917, -5.750104904174805, -4.095053195953369, -2.4263410568237305, -4.020720481872559, -6.258679389953613, -6.072696685791016, 1.9007062911987305, -1.4942448139190674, -6.954280376434326, -3.5148544311523438, -5.964104652404785, -5.208161354064941, -5.524476051330566, -4.3573079109191895, -4.765958786010742, -4.64605188369751, -4.644428253173828, -7.421418190002441, -6.2081990242004395, -4.391549110412598, -6.13762092590332, -6.177700519561768, -7.032096862792969, -6.522231101989746, -5.776354789733887, -5.097567558288574, -6.701676368713379, -6.936709880828857, -7.508005619049072, -7.092804908752441, -6.688174247741699, -7.386406421661377, -7.347876071929932, -6.672183036804199, -8.151607513427734, -4.478097915649414, -7.867572784423828, -7.298786640167236, -7.456561088562012, -6.588874816894531, -6.158859729766846, -6.003811836242676, -6.195713043212891, -6.281411647796631, -6.7042236328125, -6.330900192260742, -6.456168174743652, -7.242720603942871, -7.200074195861816, -6.988475322723389, -4.70040225982666, -7.5146942138671875, -7.031817436218262, -6.117115497589111, -6.434101104736328, -5.481025695800781, -6.507185935974121, -6.4319329261779785, -4.176630020141602, -7.51243782043457, -6.926205158233643, -6.541365146636963, -7.3740692138671875, -7.193652153015137, -7.105874061584473, -4.308794021606445, -7.838356018066406, -6.632574081420898, -6.441076278686523, -3.719160795211792, -5.674831390380859, -5.6850481033325195, -2.114366054534912, -4.72206974029541, -5.512267589569092, -7.974374294281006, -6.712493896484375, -7.569369316101074, -7.476883888244629, -6.601122856140137, -6.69597053527832, -6.434848308563232, -6.800774574279785, -7.390445709228516, -6.542190074920654, -6.638566493988037, -6.557436943054199, -6.852815628051758, -7.07601261138916, -7.377858638763428, -7.093201160430908, -6.472635269165039, -6.809480667114258, -7.4260663986206055, -7.258027076721191, -5.294919013977051, -7.542479515075684, -7.243736267089844, -6.225769996643066, -6.796802520751953, -5.954415321350098, -6.974846839904785, -6.497372150421143, -6.415834903717041, -6.055720329284668, -7.0385026931762695, -6.661272048950195, -6.525211334228516, -6.771136283874512, -7.331887722015381, -6.3608245849609375, -7.695714950561523, -6.508565425872803, -4.8772501945495605, -6.734592914581299, -7.232966899871826, -4.489096164703369, -6.191707611083984, -6.206180095672607, -7.079996585845947, -7.349147796630859, -4.868086814880371, -6.626120567321777, -5.99623966217041, -5.1211419105529785, -6.292483806610107, -6.573369979858398, -4.0575127601623535, -7.047492504119873, -6.622360706329346, -4.670816421508789, -6.023917198181152, -6.089078426361084, -5.9351654052734375, -5.11582088470459, -7.158534049987793, -6.707010269165039, -6.6321868896484375, -7.427767753601074, -7.727116584777832, -7.389705657958984, -7.249917984008789, -7.198233604431152, -7.4391984939575195, -7.416876792907715, -7.845714569091797, -5.340607643127441, -7.691741466522217, -7.632021427154541, -5.799671649932861, -6.812046051025391, -7.05814790725708, -7.306992053985596, -6.800338268280029, -6.18418025970459, -7.5804901123046875, -6.9222798347473145, -6.996889114379883, -6.215412616729736, -6.982117652893066, -5.991657257080078, -1.4492602348327637, 0.4294601082801819, -3.9610061645507812, -7.619678020477295, -7.808478355407715, -7.041165828704834, -6.697868347167969, -7.353612899780273, -7.622521877288818, -7.968989372253418, -7.322917938232422, -7.063151836395264, -7.371765613555908, -7.792178153991699, -7.789904594421387, -7.267242431640625, -5.275234222412109, -7.040386199951172, -7.716383934020996, -7.085010528564453, -6.839468955993652, -6.588361740112305, -7.086066246032715, -7.471796989440918, -7.720139503479004, -6.602481842041016, -6.309969902038574, -6.54976224899292, -6.082326889038086, -6.711950302124023, 1.506089210510254, -6.676960468292236, -7.90441370010376, -7.929075241088867, -7.905246257781982, -7.907382011413574, -7.923370838165283, -7.895792007446289, -7.883852958679199, -7.895256519317627, -7.856805801391602, -7.873743534088135, -7.899471282958984, -7.915332794189453, -7.895209312438965, -7.912853240966797, -7.891242504119873, -7.875715732574463, -7.8886518478393555, -7.858083248138428, -7.842825889587402, -7.85215950012207, -7.836614608764648, -7.830417156219482, -7.843218803405762, -7.854541301727295, -7.87191915512085, -7.930305480957031, -7.889933109283447, -7.859598636627197, -7.864480495452881, -7.874314308166504, -7.8824310302734375, -7.85033655166626, -7.877160549163818, -7.875991344451904, -7.851661682128906, -7.942914962768555, -7.918953895568848, -7.867791175842285, -7.8745832443237305, -7.915678024291992, -7.862862586975098, -7.907220840454102, -7.880338668823242, -7.874671936035156, -7.875723838806152, -7.9104719161987305, -7.922780990600586, -7.863136291503906, -7.883433818817139, -7.8603105545043945, -7.862286567687988, -7.852269172668457, -7.849606037139893, -7.856163024902344, -7.862124443054199, -7.858534812927246, -7.861769676208496, -7.859824180603027, -7.874240875244141, -7.87076473236084, -7.8723249435424805, -7.870092391967773, -7.872854709625244, -7.877506256103516, -7.881562232971191, -7.870395660400391, -7.907867431640625, -7.883175373077393, -7.866803169250488, -7.855776309967041, -7.8396992683410645, -7.859709739685059, -7.831795692443848, -7.8386688232421875, -7.865429401397705, -7.844372749328613, -7.876314163208008, -7.880326271057129, -7.8718767166137695, -7.864551544189453, -7.860812187194824, -7.87119722366333, -7.875679969787598, -7.882021903991699, -7.849362850189209, -7.863656997680664, -7.861644268035889, -7.876238822937012, -7.861681938171387, -7.858334541320801, -7.887132167816162, -7.902436256408691, -7.848311901092529, -7.8622307777404785, -7.931234836578369, -7.929795265197754, -7.870090484619141, -7.841987133026123, -7.845444202423096, -7.859190940856934, -7.859401702880859, -7.854689598083496, -7.83934211730957, -7.838196277618408, -7.82147741317749, -7.836581230163574, -7.825407981872559, -7.876710891723633, -7.84513521194458, -7.8816070556640625, -7.870439052581787, -7.870837688446045, -7.890906810760498, -7.875052452087402, -7.907040596008301, -7.893671989440918, -7.89454984664917, -7.905453681945801, -7.892573833465576, -7.9104437828063965, -7.9108428955078125, -7.907593250274658, -7.894451141357422, -7.894471645355225, -7.917994499206543, -7.933116436004639, -7.902120590209961, -7.928289413452148, -7.930464744567871, -7.894152641296387, -7.890780925750732, -7.908388614654541, -7.913780212402344, -7.92290735244751, -7.9204607009887695, -7.913249969482422, -7.923950672149658, -7.929731369018555, -7.924129009246826, -7.925617694854736, -7.916412353515625, -7.941255569458008, -7.944853782653809, -7.957110404968262, -7.967599868774414, -7.962024688720703, -7.96016788482666, -7.953985214233398, -7.962321758270264, -7.945852756500244, -7.9215898513793945, -7.925055503845215, -7.938163757324219, -7.932643413543701, -7.912038803100586, -7.898999214172363, -7.910369873046875, -7.905282020568848, -7.909702301025391, -7.908716678619385, -7.905632972717285, -7.900643825531006, -7.901747703552246, -7.89312744140625, -7.890666484832764, -7.905547142028809, -7.93037223815918]\n",
            "Best start position : 24\n",
            "End logits : [-3.9674155712127686, -6.100870132446289, -6.39224910736084, -6.531193256378174, -6.7228522300720215, -6.313442707061768, -6.982839584350586, -3.5287904739379883, -3.7518701553344727, -3.7780632972717285, -1.8113007545471191, -2.145296335220337, -3.977975845336914, -4.739995956420898, -3.4965648651123047, -4.606476306915283, -5.280902862548828, -5.056633472442627, -2.9848670959472656, -2.6316421031951904, -2.7101802825927734, -1.9409396648406982, -3.243572950363159, -1.6340317726135254, 5.366088390350342, 2.1301567554473877, 0.09167224168777466, 0.6002203226089478, 1.1888142824172974, -1.282031536102295, -3.297910690307617, -4.914864540100098, -3.6784040927886963, -4.329772472381592, -5.301434516906738, -5.232013702392578, -3.2340171337127686, -4.779553413391113, -4.745108604431152, -5.964869976043701, -4.428951263427734, -5.288202285766602, -4.575783729553223, -4.448217391967773, -5.6916608810424805, -4.774053573608398, -5.974774360656738, -5.06923246383667, -3.628913402557373, -3.821209192276001, -3.811089038848877, -4.471020698547363, -2.463815450668335, -3.196943759918213, -5.962147235870361, -5.805627822875977, -4.193437099456787, -4.5340752601623535, -4.664980888366699, -3.4920153617858887, -5.65885066986084, -5.425605773925781, -5.453238487243652, -6.299984931945801, -4.863497734069824, -5.17752742767334, -5.818402290344238, -4.439551830291748, -4.849819183349609, -2.7755041122436523, -5.002986907958984, -4.525267601013184, -5.287436485290527, -4.321990966796875, -4.4156341552734375, -3.6971540451049805, -4.573956489562988, -4.412574291229248, -4.225841999053955, -5.636496067047119, -4.006952285766602, -4.573182106018066, -4.806406021118164, -5.223947525024414, -3.8163230419158936, -3.6071295738220215, -3.8175525665283203, -4.608965873718262, -3.4163942337036133, -3.2486538887023926, -3.8007867336273193, -3.9272522926330566, -2.0746164321899414, -4.001475811004639, -4.302412509918213, -2.1398050785064697, -2.612907886505127, -4.444369792938232, -4.017675876617432, 2.2845664024353027, -1.9625016450881958, -4.134365558624268, -5.162029266357422, -3.180793285369873, -2.3857362270355225, -5.608682632446289, -5.147611618041992, -4.875818252563477, -3.59684681892395, -2.919738292694092, -2.6774168014526367, -3.7360095977783203, 2.3430838584899902, -2.5690064430236816, -3.331209182739258, -0.8638136386871338, -2.560992956161499, -0.1798122227191925, -4.121283054351807, -4.423851013183594, -6.349506855010986, -4.946193695068359, -3.486419677734375, -2.3389971256256104, -5.630119323730469, -3.1350903511047363, -5.306652069091797, -3.246000289916992, -4.894207954406738, -4.281908988952637, -4.786675453186035, -3.352485418319702, -5.53998327255249, -6.213830471038818, -3.633716106414795, -4.438375473022461, -4.665489196777344, -6.636274337768555, -6.0876383781433105, -6.028399467468262, -4.589858055114746, -4.279862880706787, -4.732676982879639, -5.8703413009643555, -4.736845016479492, -5.929018020629883, -5.640880107879639, -5.930480003356934, -3.823218584060669, -5.088301658630371, -5.851467132568359, -5.192122459411621, -5.040593147277832, -5.385549545288086, -4.232882499694824, -4.042757511138916, -5.450300216674805, -3.683274507522583, 1.788304090499878, 0.16422079503536224, -3.7034215927124023, -3.3090872764587402, -5.334609508514404, -4.502371788024902, -5.306723117828369, -4.775582790374756, -4.781089782714844, -4.47598934173584, -2.54396653175354, -4.824881553649902, -6.505526065826416, -5.3737640380859375, -6.371720790863037, -6.066540241241455, -5.4279279708862305, -6.509067535400391, -4.726795196533203, -4.954977035522461, -6.3306355476379395, -5.9810709953308105, -6.295709133148193, -7.234332084655762, -6.798330307006836, -6.518129825592041, -6.230881690979004, -5.52386999130249, -6.529546737670898, -4.171308994293213, -6.570934295654297, -6.671131610870361, -6.627926826477051, -7.242568016052246, -6.710119724273682, -6.578012943267822, -6.546243667602539, -6.634181022644043, -6.550995826721191, -6.396318435668945, -5.714037895202637, -5.662947177886963, -5.6991190910339355, -6.371121406555176, -5.328307628631592, -7.343006610870361, -6.786751747131348, -5.580699920654297, -6.822132587432861, -5.8185529708862305, -5.813222885131836, -6.300905227661133, -4.375896453857422, -7.204519271850586, -5.551597595214844, -5.946556568145752, -6.9020466804504395, -7.362592697143555, -7.230646133422852, -4.293928146362305, -7.229863166809082, -7.252713680267334, -7.283926010131836, -3.9439945220947266, -5.363433837890625, -6.168872356414795, -2.0561978816986084, -4.746047019958496, -4.226506233215332, -6.370004653930664, -6.655373573303223, -6.552124977111816, -6.422806739807129, -6.706055641174316, -6.707969665527344, -6.363865852355957, -6.7692670822143555, -6.915016174316406, -6.104605674743652, -5.710354804992676, -6.571249485015869, -6.207308292388916, -5.6290740966796875, -6.100906848907471, -6.866706371307373, -6.349128723144531, -6.846013069152832, -6.916843891143799, -6.971591472625732, -5.84967041015625, -7.433524131774902, -6.965445041656494, -5.785870552062988, -6.949216842651367, -5.552278518676758, -5.7168121337890625, -6.188132286071777, -6.653017520904541, -5.846096038818359, -6.814796447753906, -7.245854377746582, -7.006346702575684, -6.760768413543701, -7.077442646026611, -5.494626045227051, -6.398309707641602, -6.703070640563965, -5.095096588134766, -6.795690536499023, -6.6594696044921875, -4.706719398498535, -5.965357303619385, -5.727630615234375, -5.997524261474609, -6.424256801605225, -5.901818752288818, -5.8889055252075195, -6.284400939941406, -5.265749931335449, -5.047256946563721, -6.74504280090332, -4.926085948944092, -7.06951379776001, -6.318333625793457, -4.671696662902832, -5.738192558288574, -6.6261091232299805, -6.545579433441162, -4.005002498626709, -6.039914131164551, -7.561335563659668, -7.417447090148926, -7.322559356689453, -7.528675079345703, -7.067590236663818, -6.96317195892334, -7.002159118652344, -6.635446548461914, -6.364005088806152, -7.083967685699463, -5.321065425872803, -6.9552531242370605, -6.79566764831543, -6.02225399017334, -6.784082412719727, -6.5198283195495605, -6.611688613891602, -7.450405120849609, -7.038026809692383, -7.000111103057861, -7.178754806518555, -7.02367639541626, -6.799008369445801, -6.23325777053833, -6.487602233886719, -2.7605652809143066, 0.1857222467660904, -3.0416622161865234, -6.977972030639648, -5.917288780212402, -6.518028736114502, -6.890661239624023, -7.345150470733643, -7.476180553436279, -7.482940196990967, -6.937047958374023, -6.573564052581787, -5.9098801612854, -6.582416534423828, -6.643436431884766, -7.142393112182617, -5.469860553741455, -6.851955890655518, -7.00813102722168, -7.513747215270996, -7.33367395401001, -6.918752670288086, -7.131341934204102, -6.529086589813232, -7.112728118896484, -7.248910427093506, -7.070683479309082, -7.054116249084473, -4.97779655456543, -5.274360656738281, 1.3822963237762451, -5.375852584838867, -7.767526149749756, -7.7671966552734375, -7.770484447479248, -7.776995658874512, -7.763345718383789, -7.754364967346191, -7.741883277893066, -7.760389804840088, -7.739602088928223, -7.745510578155518, -7.738387107849121, -7.751330375671387, -7.751332759857178, -7.750896453857422, -7.760191440582275, -7.765010833740234, -7.754204750061035, -7.770455360412598, -7.801170349121094, -7.797024250030518, -7.812160968780518, -7.833059787750244, -7.83435583114624, -7.815265655517578, -7.80167293548584, -7.778190612792969, -7.784148693084717, -7.791365623474121, -7.7900390625, -7.768376350402832, -7.787191390991211, -7.7690887451171875, -7.747206687927246, -7.753177165985107, -7.762138366699219, -7.778698444366455, -7.763517379760742, -7.75205135345459, -7.775896072387695, -7.7766900062561035, -7.773965835571289, -7.7829461097717285, -7.800023078918457, -7.808432102203369, -7.796008586883545, -7.785466194152832, -7.7710347175598145, -7.802936553955078, -7.779829502105713, -7.797865867614746, -7.795461177825928, -7.79788875579834, -7.792670726776123, -7.778723239898682, -7.789823055267334, -7.782752513885498, -7.7920823097229, -7.77424430847168, -7.7937421798706055, -7.786364555358887, -7.810066223144531, -7.823808670043945, -7.787637710571289, -7.82481575012207, -7.829504013061523, -7.809687614440918, -7.77842903137207, -7.778244495391846, -7.790703773498535, -7.80002498626709, -7.784310340881348, -7.7617292404174805, -7.777980804443359, -7.767158031463623, -7.7420501708984375, -7.765235900878906, -7.742902755737305, -7.758968830108643, -7.7710041999816895, -7.7578125, -7.782253265380859, -7.776904106140137, -7.7890729904174805, -7.800253868103027, -7.7723236083984375, -7.780289649963379, -7.784842491149902, -7.805646896362305, -7.828944206237793, -7.826638221740723, -7.780271530151367, -7.7783966064453125, -7.8293256759643555, -7.797008991241455, -7.77858304977417, -7.775092124938965, -7.799999713897705, -7.837965965270996, -7.833304405212402, -7.8174943923950195, -7.81528377532959, -7.808155059814453, -7.8242950439453125, -7.802398681640625, -7.813217639923096, -7.785655975341797, -7.802736282348633, -7.790040493011475, -7.78752326965332, -7.781838417053223, -7.770966529846191, -7.782978057861328, -7.771286964416504, -7.771883487701416, -7.775969505310059, -7.7644548416137695, -7.7552924156188965, -7.755242347717285, -7.749300003051758, -7.742037773132324, -7.739224910736084, -7.74252462387085, -7.734774112701416, -7.75168514251709, -7.7944159507751465, -7.745473861694336, -7.736884117126465, -7.779126167297363, -7.777013778686523, -7.726795673370361, -7.739995956420898, -7.7419047355651855, -7.762304306030273, -7.766039848327637, -7.752037048339844, -7.762691497802734, -7.754650115966797, -7.764172554016113, -7.752412796020508, -7.739438056945801, -7.754364967346191, -7.761116981506348, -7.753453254699707, -7.78698205947876, -7.804319858551025, -7.802666664123535, -7.7980451583862305, -7.7741804122924805, -7.787935256958008, -7.780144691467285, -7.780473709106445, -7.7769365310668945, -7.7753753662109375, -7.792657852172852, -7.767960548400879, -7.780468940734863, -7.779630661010742, -7.767148971557617, -7.776123046875, -7.762284755706787, -7.769173622131348, -7.7659912109375, -7.760531425476074, -7.764806747436523, -7.768082141876221, -7.765305042266846, -7.76817512512207]\n",
            "Best end position : 24\n",
            "Yes score : [-1.9006447792053223]\n",
            "No score : [-1.8907781839370728]\n",
            "Unknown score : [0.6426644921302795]\n",
            "white\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we can evaluate our answers with the official evaluation script.\n",
        "\n",
        "This is stored in processors/eval.py\n",
        "\n",
        "We should have an exact match (em) score of around 65% and an F1 score of around 75%."
      ],
      "metadata": {
        "id": "L7ypzVEu2sU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from processors.eval import CoQAEvaluator\n",
        "\n",
        "def print_results():\n",
        "    evaluator = CoQAEvaluator(\"./data/coqa-dev-v1.0.json\")\n",
        "    prediction_file = os.path.join(\"./weights\", \"predictions.json\")\n",
        "    with open(prediction_file) as f:\n",
        "        pred_data = CoQAEvaluator.preds_to_dict(prediction_file)\n",
        "    print(json.dumps(evaluator.model_performance(pred_data), indent=2))\n",
        "\n",
        "print_results()\n"
      ],
      "metadata": {
        "id": "f5FWWqC_24qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we can use our very own story and questions to get desired answers.\n",
        "\n",
        "We have to go through the whole pipeline to convert our story, question to tensors, run inference and decode output."
      ],
      "metadata": {
        "id": "mTHJcEKR30Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Story = \"\"\"In the hushed confines of a cutting-edge particle physics laboratory, Dr. Samuel Reynolds and his team embarked on a journey to uncover the mysteries of the universe.\n",
        "They sought to unravel the enigma of dark matter, an invisible substance that eluded comprehension for centuries.\n",
        "One fateful day, as the team conducted an experiment involving the behavior of subatomic particles, they noticed an anomaly.\n",
        "Particles interacted in a peculiar manner, exhibiting an inexplicable attraction to one another.\n",
        "Dr. Reynolds named this uncharted force the \"Cryptic Force,\" symbolizing the hidden secrets it promised to unveil.\n",
        "\n",
        "The discovery sent shockwaves through the scientific community.\n",
        "Skepticism abounded initially, but subsequent experiments replicated the results.\n",
        "The Cryptic Force, distinct from the four known fundamental forces, held the potential to bridge gaps in our understanding of the cosmos.\n",
        "Cryptic Force seemed to be intertwined with dark matter, offering a glimmer of hope in solving the puzzle of the universe's invisible enigma.\n",
        "The revelation ignited new lines of inquiry into the nature of dark matter, as scientists worldwide raced to unlock its secrets.\"\"\"\n",
        "\n",
        "Question = \"Who discovered the cryptic force?\"\n",
        "\n",
        "input = {'story':Story,\n",
        "         'questions':[{'turn_id':0, 'input_text':Question}],\n",
        "         'answers':[{'turn_id':0, 'span_start':0, 'span_end':0, 'span_text':\"\", 'input_text':\"\"}],\n",
        "         'source':\"Me\", 'filename':\"Me\", 'id':\"Me\"}\n",
        "\n",
        "my_example = Processor()._create_examples(input, 1)\n",
        "print(my_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vh8H0B64HbP",
        "outputId": "984eb445-5221-48f1-84fb-368905d3bda9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Question : ['Who discovered the cryptic force?']\n",
            "                   Document : ['In', 'the', 'hushed', 'confines', 'of', 'a', 'cutting', '-', 'edge', 'particle', 'physics', 'laboratory', ',', 'Dr.', 'Samuel', 'Reynolds', 'and', 'his', 'team', 'embarked', 'on', 'a', 'journey', 'to', 'uncover', 'the', 'mysteries', 'of', 'the', 'universe', '.', 'They', 'sought', 'to', 'unravel', 'the', 'enigma', 'of', 'dark', 'matter', ',', 'an', 'invisible', 'substance', 'that', 'eluded', 'comprehension', 'for', 'centuries', '.', 'One', 'fateful', 'day', ',', 'as', 'the', 'team', 'conducted', 'an', 'experiment', 'involving', 'the', 'behavior', 'of', 'subatomic', 'particles', ',', 'they', 'noticed', 'an', 'anomaly', '.', 'Particles', 'interacted', 'in', 'a', 'peculiar', 'manner', ',', 'exhibiting', 'an', 'inexplicable', 'attraction', 'to', 'one', 'another', '.', 'Dr.', 'Reynolds', 'named', 'this', 'uncharted', 'force', 'the', '\"', 'Cryptic', 'Force', ',', '\"', 'symbolizing', 'the', 'hidden', 'secrets', 'it', 'promised', 'to', 'unveil', '.', 'The', 'discovery', 'sent', 'shockwaves', 'through', 'the', 'scientific', 'community', '.', 'Skepticism', 'abounded', 'initially', ',', 'but', 'subsequent', 'experiments', 'replicated', 'the', 'results', '.', 'The', 'Cryptic', 'Force', ',', 'distinct', 'from', 'the', 'four', 'known', 'fundamental', 'forces', ',', 'held', 'the', 'potential', 'to', 'bridge', 'gaps', 'in', 'our', 'understanding', 'of', 'the', 'cosmos', '.', 'Cryptic', 'Force', 'seemed', 'to', 'be', 'intertwined', 'with', 'dark', 'matter', ',', 'offering', 'a', 'glimmer', 'of', 'hope', 'in', 'solving', 'the', 'puzzle', 'of', 'the', 'universe', \"'s\", 'invisible', 'enigma', '.', 'The', 'revelation', 'ignited', 'new', 'lines', 'of', 'inquiry', 'into', 'the', 'nature', 'of', 'dark', 'matter', ',', 'as', 'scientists', 'worldwide', 'raced', 'to', 'unlock', 'its', 'secrets', '.']\n",
            "                   Ground Truth : \n",
            "                   Answer Span : 0 -> 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to tensors"
      ],
      "metadata": {
        "id": "IVNJDKTm84fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset, my_features = load_dataset(my_example, tokenizer, evaluate = True)\n",
        "my_features = my_features[0]\n",
        "print(my_features.tokens)\n",
        "\n",
        "inp = {'input_ids':torch.tensor([my_features.input_ids]).to(device),\n",
        "       \"segment_ids\": torch.tensor([my_features.segment_ids]).to(device),\n",
        "       \"input_masks\": torch.tensor([my_features.input_mask]).to(device)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px8Y0clS9BDt",
        "outputId": "e5878fa3-d87a-4a9d-ce1d-dc1b5bda0d61"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features from dataset: 100%|██████████| 1/1 [00:00<00:00, 22.96it/s]\n",
            "Tag unique id to each example: 100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'who', 'discovered', 'the', 'cryptic', 'force', '?', '[SEP]', 'in', 'the', 'hushed', 'confines', 'of', 'a', 'cutting', '-', 'edge', 'particle', 'physics', 'laboratory', ',', 'dr', '.', 'samuel', 'reynolds', 'and', 'his', 'team', 'embarked', 'on', 'a', 'journey', 'to', 'uncover', 'the', 'mysteries', 'of', 'the', 'universe', '.', 'they', 'sought', 'to', 'un', '##rave', '##l', 'the', 'enigma', 'of', 'dark', 'matter', ',', 'an', 'invisible', 'substance', 'that', 'el', '##uded', 'comprehension', 'for', 'centuries', '.', 'one', 'fate', '##ful', 'day', ',', 'as', 'the', 'team', 'conducted', 'an', 'experiment', 'involving', 'the', 'behavior', 'of', 'sub', '##ato', '##mic', 'particles', ',', 'they', 'noticed', 'an', 'anomaly', '.', 'particles', 'interact', '##ed', 'in', 'a', 'peculiar', 'manner', ',', 'exhibiting', 'an', 'in', '##ex', '##pl', '##ica', '##ble', 'attraction', 'to', 'one', 'another', '.', 'dr', '.', 'reynolds', 'named', 'this', 'un', '##cha', '##rted', 'force', 'the', '\"', 'cryptic', 'force', ',', '\"', 'symbol', '##izing', 'the', 'hidden', 'secrets', 'it', 'promised', 'to', 'un', '##ve', '##il', '.', 'the', 'discovery', 'sent', 'shock', '##wave', '##s', 'through', 'the', 'scientific', 'community', '.', 'skepticism', 'ab', '##ounded', 'initially', ',', 'but', 'subsequent', 'experiments', 'replicate', '##d', 'the', 'results', '.', 'the', 'cryptic', 'force', ',', 'distinct', 'from', 'the', 'four', 'known', 'fundamental', 'forces', ',', 'held', 'the', 'potential', 'to', 'bridge', 'gaps', 'in', 'our', 'understanding', 'of', 'the', 'cosmos', '.', 'cryptic', 'force', 'seemed', 'to', 'be', 'inter', '##twined', 'with', 'dark', 'matter', ',', 'offering', 'a', 'g', '##lim', '##mer', 'of', 'hope', 'in', 'solving', 'the', 'puzzle', 'of', 'the', 'universe', \"'\", 's', 'invisible', 'enigma', '.', 'the', 'revelation', 'ignited', 'new', 'lines', 'of', 'inquiry', 'into', 'the', 'nature', 'of', 'dark', 'matter', ',', 'as', 'scientists', 'worldwide', 'raced', 'to', 'unlock', 'its', 'secrets', '.', '[SEP]']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we can pass it through the model and get logits.\n",
        "\n"
      ],
      "metadata": {
        "id": "QVSPEsTT9A1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outp = trained_model(**inp)\n",
        "outp = [o.detach().cpu().tolist()[0] for o in outp]\n",
        "start_logits, end_logits, yes_logits, no_logits, unk_logits = outp\n",
        "print(start_logits)\n",
        "print(end_logits)\n",
        "print(yes_logits)\n",
        "print(no_logits)\n",
        "print(unk_logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEG0HFNe-aEt",
        "outputId": "d26c5929-3c5a-4d90-c4d4-1c516980077a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.0385701656341553, -4.43826961517334, -6.55425500869751, -7.164093017578125, -7.3818206787109375, -7.840551376342773, -4.996629238128662, -6.9712443351745605, -0.9702745676040649, -3.4593305587768555, -4.290556907653809, -5.0918169021606445, -4.2428483963012695, -0.8664641380310059, -3.257561683654785, -5.441644668579102, -6.145437717437744, -1.9303851127624512, -3.901210308074951, -3.3312127590179443, -2.6177828311920166, 7.235795021057129, -0.4374772012233734, 4.472531318664551, 1.6176892518997192, 1.6589255332946777, 2.968040943145752, 2.080980062484741, -1.4893442392349243, -3.5463972091674805, -1.8986639976501465, -3.786787509918213, -2.7399799823760986, -3.836890459060669, -2.2709193229675293, -4.152251720428467, -4.979352951049805, -2.576104164123535, -4.519848823547363, -6.162850856781006, -0.3449029326438904, -4.240826606750488, -3.511794090270996, -4.128286838531494, -6.265950679779053, -6.257771968841553, -2.7426671981811523, -3.521883964538574, -5.332711219787598, -1.7956228256225586, -5.582730293273926, -6.534538269042969, -4.359191417694092, -4.419366359710693, -5.903697490692139, -5.737481594085693, -5.743677616119385, -5.700829982757568, -5.869329452514648, -5.815407752990723, -5.469330310821533, -6.9206438064575195, -2.227417230606079, -5.0684380531311035, -6.506234169006348, -4.939265251159668, -5.9220685958862305, -3.8257784843444824, -1.3633933067321777, -0.3946557641029358, -4.342355728149414, -3.2638795375823975, -4.415897846221924, -4.8535356521606445, -3.334307909011841, -4.062134742736816, -5.654489040374756, -2.5778393745422363, -6.70867395401001, -6.581106662750244, -4.543202877044678, -7.1912150382995605, -0.7990128993988037, -4.338625907897949, -3.882451057434082, -5.165134906768799, -7.0348381996154785, -5.223525524139404, -6.973264217376709, -7.030767440795898, -5.730737209320068, -6.6089701652526855, -6.784300804138184, -7.642962455749512, -7.548440933227539, -6.155200958251953, -6.197196960449219, -6.034829616546631, -7.225416660308838, -7.625092506408691, -7.493684768676758, -7.530613899230957, -6.868278503417969, -6.450690269470215, -6.1823225021362305, -7.418635368347168, -6.714489459991455, 3.6194067001342773, -3.1026928424835205, -0.536505937576294, -2.0372602939605713, -2.958388090133667, -3.9800679683685303, -6.21181058883667, -6.391637802124023, -4.0465087890625, -3.854243040084839, -3.6976089477539062, -2.622774362564087, -5.392370223999023, -6.687162399291992, -5.296443462371826, -5.083244800567627, -6.9473981857299805, -5.973012447357178, -5.309981346130371, -6.4910359382629395, -4.831363201141357, -6.96101713180542, -6.916853904724121, -6.980865478515625, -7.23558235168457, -7.883764266967773, -7.234653472900391, -3.6979622840881348, -4.498458385467529, -5.15186882019043, -4.867012023925781, -6.775111198425293, -7.028480529785156, -5.804176330566406, -2.7364585399627686, -3.1350975036621094, -5.5770583152771, -6.715554237365723, -5.924361228942871, -6.898749351501465, -7.603198528289795, -6.954739570617676, -8.0604887008667, -5.67117977142334, -4.249356746673584, -5.1029438972473145, -6.55421257019043, -6.985232353210449, -6.590068817138672, -7.168997764587402, -6.762164115905762, -3.6522457599639893, -3.551801919937134, -5.063832759857178, -5.741828918457031, -4.047544002532959, -4.516793727874756, -4.484460830688477, -2.3246333599090576, -5.052495002746582, -4.401170253753662, -6.065874099731445, -7.176515102386475, -4.153204917907715, -5.342737197875977, -5.529904842376709, -6.01108455657959, -5.395555019378662, -6.384265899658203, -6.892955780029297, -6.596758842468262, -6.769837379455566, -7.319798469543457, -6.663330078125, -6.93112850189209, -7.275629043579102, -4.204696178436279, -5.581115245819092, -3.8823137283325195, -5.070321559906006, -5.264132499694824, -4.9594035148620605, -6.900002479553223, -5.347721099853516, -4.028631687164307, -6.512692451477051, -7.836373805999756, -5.827620029449463, -6.194334506988525, -6.466909408569336, -7.45743989944458, -7.644617557525635, -7.569050312042236, -7.050727844238281, -6.562283039093018, -6.6619672775268555, -6.550283908843994, -6.5706706047058105, -7.526378631591797, -6.107156753540039, -6.69176721572876, -7.848252296447754, -7.820930480957031, -6.110470771789551, -6.927414417266846, -7.566939353942871, -4.70435905456543, -6.141266822814941, -6.4090471267700195, -5.0514702796936035, -6.626660346984863, -7.530417442321777, -6.871589183807373, -6.22415018081665, -5.828927993774414, -6.0922017097473145, -7.439160346984863, -5.32914924621582, -6.915364742279053, -8.273521423339844, -5.899605751037598, -0.6068413853645325, -4.154211521148682, -6.437309265136719, -6.399135112762451, -6.1915154457092285, -6.441479206085205, -6.395049095153809, 0.4185003638267517, -5.943763732910156, -7.810843467712402, -7.809977054595947, -7.8108344078063965, -7.810750961303711, -7.829794883728027, -7.805821418762207, -7.8153533935546875, -7.8037543296813965, -7.795480251312256, -7.798703193664551, -7.804781913757324, -7.808237075805664, -7.8050384521484375, -7.825700759887695, -7.830596446990967, -7.8260111808776855, -7.837276935577393, -7.84810733795166, -7.829592704772949, -7.838644027709961, -7.8201751708984375, -7.812676429748535, -7.846826553344727, -7.796404838562012, -7.836904048919678, -7.828611850738525, -7.801609039306641, -7.816576957702637, -7.771291732788086, -7.7985639572143555, -7.821847438812256, -7.835956573486328, -7.830244541168213, -7.823541641235352, -7.8310747146606445, -7.8141770362854, -7.818427085876465, -7.791136741638184, -7.8058695793151855, -7.806910991668701, -7.790223598480225, -7.793521881103516, -7.812317371368408, -7.837939262390137, -7.821264743804932, -7.825141429901123, -7.80225944519043, -7.856105327606201, -7.853326797485352, -7.85137939453125, -7.918499946594238, -7.893713474273682, -7.8876237869262695, -7.920675277709961, -7.879794597625732, -8.072632789611816, -7.985651969909668, -7.992687225341797, -7.917904376983643, -7.853498458862305, -7.833667278289795, -7.856239318847656, -7.833343505859375, -7.794550895690918, -7.7997660636901855, -7.8634538650512695, -7.84887170791626, -7.890825271606445, -7.839942932128906, -7.90083122253418, -7.871590614318848, -7.859730243682861, -7.857648849487305, -7.819307804107666, -7.8127360343933105, -7.831162452697754, -7.81882381439209, -7.83672571182251, -7.797299861907959, -7.78969669342041, -7.822011470794678, -7.815661430358887, -7.790901184082031, -7.799323558807373, -7.846784591674805, -7.833324432373047, -7.857294082641602, -7.84846305847168, -7.826818466186523, -7.8045783042907715, -7.801329612731934, -7.828488349914551, -7.801249027252197, -7.840688228607178, -7.839710712432861, -7.872615814208984, -7.78617000579834, -7.797031402587891, -7.7981390953063965, -7.822256565093994, -7.818388938903809, -7.831782341003418, -7.817819118499756, -7.825161933898926, -7.803456783294678, -7.815030097961426, -7.7964396476745605, -7.887813568115234, -7.8437042236328125, -7.932228088378906, -7.898031711578369, -7.861376762390137, -7.788289546966553, -7.7987189292907715, -7.799293041229248, -7.836822509765625, -7.7910475730896, -7.79649019241333, -7.833187580108643, -7.8277907371521, -7.817622661590576, -7.818087577819824, -7.802699565887451, -7.8097615242004395, -7.828029632568359, -7.853974342346191, -7.800986289978027, -7.791236400604248, -7.8275370597839355, -7.812176704406738, -7.851886749267578, -7.844019889831543, -7.81730318069458, -7.814870357513428, -7.860292434692383, -7.818961143493652, -7.8602752685546875, -7.789251804351807, -7.882295608520508, -7.805834770202637, -7.840795993804932, -7.797644138336182, -7.790991306304932, -7.864047050476074, -7.908137798309326, -7.837773323059082, -7.824408531188965, -7.8737287521362305, -7.858131408691406, -7.803300380706787, -7.813173770904541, -7.819568634033203, -7.823748588562012, -7.8396830558776855, -7.78312349319458, -7.828320026397705, -7.772360801696777, -7.748187065124512, -7.787084579467773, -7.77759313583374, -7.816906452178955, -7.818569183349609, -7.832852363586426, -7.803203582763672, -7.79408073425293, -7.820960998535156, -7.879145622253418, -7.840997219085693, -7.875799179077148, -7.869711875915527, -7.856279373168945, -7.872020244598389, -7.8964619636535645, -7.936101913452148, -7.911000728607178, -7.881765365600586, -7.833375453948975, -7.816875457763672, -7.801636219024658, -7.816397666931152, -7.867864608764648, -7.80789041519165, -7.866600036621094, -7.8742146492004395, -7.85404109954834, -7.810394763946533, -7.816656112670898, -7.891511917114258, -7.92523193359375, -7.777174949645996, -7.8318705558776855, -7.890915393829346, -7.777885913848877, -7.884383201599121, -7.829422950744629, -7.895325660705566, -7.862818241119385, -7.897224426269531, -7.862668991088867, -7.863992691040039, -7.847347259521484, -7.928889274597168, -7.926866054534912, -7.888807773590088, -7.861588001251221, -7.880765438079834, -7.880772113800049, -7.888099670410156, -7.885797500610352, -7.857152462005615, -7.80251932144165, -7.842733860015869, -7.8258771896362305, -7.820941925048828, -7.811647891998291, -7.826105117797852, -7.811678886413574, -7.833686351776123, -7.855838298797607, -7.861900806427002, -7.794309139251709, -7.884376049041748, -7.8901472091674805, -7.87858772277832, -7.875253200531006, -7.999067783355713, -8.016321182250977, -7.962977886199951, -7.829862117767334, -7.828526973724365, -7.852374076843262, -7.807417392730713, -7.828333854675293, -7.807466983795166, -7.787233829498291, -7.81446647644043, -7.832586288452148, -7.841928958892822, -7.817663669586182, -7.822937488555908, -7.801956653594971, -7.819102764129639, -7.797616958618164, -7.78178071975708, -7.78776216506958, -7.815465927124023, -7.817010879516602, -7.818057060241699, -7.835301399230957, -7.860357284545898, -7.8775105476379395, -7.873190402984619, -7.883660793304443, -7.850666522979736, -7.853291988372803, -7.821435451507568, -7.823525428771973, -7.796867847442627, -7.78473424911499, -7.7786335945129395, -7.7796831130981445, -7.78655481338501, -7.7983717918396, -7.804243087768555, -7.807601451873779, -7.804131031036377, -7.810261249542236, -7.827425956726074, -7.8231329917907715, -7.8297014236450195, -7.796136379241943, -7.7888922691345215, -7.802114963531494, -7.782236099243164, -7.835902214050293]\n",
            "[-2.993422031402588, -4.371222019195557, -6.655492782592773, -7.595197677612305, -7.7486066818237305, -6.012847423553467, -4.884645938873291, -6.350613117218018, -2.0714797973632812, -4.045680522918701, -3.5209336280822754, -3.6255404949188232, -2.550173759460449, -3.1146466732025146, -5.297956466674805, -4.278092384338379, -3.442479372024536, -3.5219645500183105, -1.8198840618133545, 0.1322772055864334, 1.6420615911483765, 0.6822215914726257, 0.5209301710128784, 1.9989291429519653, 6.929810523986816, 3.4063985347747803, 2.7056403160095215, 5.221230983734131, -2.2102842330932617, -2.517591953277588, -3.1779046058654785, -2.3171000480651855, -2.4338557720184326, -3.7648589611053467, -4.186364650726318, -3.7601654529571533, -3.6082282066345215, -4.272904396057129, -0.9646055102348328, -2.3594095706939697, -0.666253387928009, -4.208251476287842, -3.8622870445251465, -5.654393672943115, -5.241127967834473, -3.206343173980713, -5.310295104980469, -3.808845043182373, -4.304692268371582, -5.343016147613525, -0.68593430519104, -2.37441349029541, -5.389256477355957, -4.870625972747803, -4.557923316955566, -4.604244232177734, -6.749021530151367, -4.615016460418701, -3.564363479614258, -5.399599552154541, -3.58371639251709, -4.1793694496154785, -2.6490166187286377, -5.129940986633301, -4.8049163818359375, -3.548137903213501, -4.076218128204346, -4.29074239730835, -3.3076958656311035, -0.40274155139923096, -4.505593776702881, -4.395886421203613, -3.8622207641601562, -5.7558698654174805, -5.991549491882324, -5.194313049316406, -5.866493225097656, -6.064859390258789, -6.8419904708862305, -5.522088050842285, -1.2554600238800049, -4.394321918487549, -1.5547568798065186, -4.69330358505249, -5.429288864135742, -3.18400239944458, -5.021735191345215, -4.945537567138672, -6.55955696105957, -5.423392295837402, -5.649198055267334, -6.533337593078613, -6.435996055603027, -5.54957389831543, -5.779646873474121, -6.483771324157715, -6.618155002593994, -6.8703532218933105, -7.138057708740234, -6.9969000816345215, -6.275015354156494, -6.206355094909668, -6.412680625915527, -6.011428356170654, -5.60795783996582, -4.797431945800781, -3.79178524017334, -0.25956863164901733, -1.186376690864563, 4.312868595123291, -2.4348490238189697, -3.798565149307251, -6.155811786651611, -5.921407699584961, -3.7323596477508545, -2.0522727966308594, -5.043673038482666, -5.368354797363281, -3.7510719299316406, -1.6904296875, -2.732541561126709, -2.87898588180542, -6.475039005279541, -5.470849990844727, -6.539695739746094, -5.748049259185791, -4.524771213531494, -4.519336223602295, -5.723353862762451, -5.643838405609131, -6.102885723114014, -5.713485240936279, -4.81287956237793, -5.038737773895264, -4.826695919036865, -4.172589302062988, -5.583103656768799, -6.154454231262207, -6.261183738708496, -4.497036933898926, -5.102519512176514, -4.118612289428711, -4.17424201965332, -1.7423659563064575, -4.966996192932129, -6.235851764678955, -7.238924980163574, -6.414231300354004, -5.579179286956787, -6.50144624710083, -6.130774021148682, -5.224569320678711, -5.0094828605651855, -6.820056438446045, -6.014277935028076, -6.984875679016113, -5.973980903625488, -6.374150276184082, -5.330245494842529, -4.207330226898193, -3.269186019897461, -4.135159492492676, -4.937708377838135, -4.575741767883301, -5.801082134246826, -2.7860891819000244, -5.150731563568115, -5.10087776184082, -2.637246608734131, -4.592751502990723, -5.9966020584106445, -6.812310695648193, -6.025285243988037, -6.78083610534668, -6.658698558807373, -6.114543914794922, -6.622915744781494, -6.412876605987549, -6.777405738830566, -6.806751251220703, -7.197108745574951, -4.8281402587890625, -5.567007541656494, -5.555757522583008, -4.326159954071045, -5.461861610412598, -5.607769966125488, -5.7642340660095215, -6.9195122718811035, -5.86887264251709, -5.53010368347168, -5.929870128631592, -3.493692636489868, -5.2980475425720215, -6.788900375366211, -6.958725929260254, -7.3830413818359375, -7.07149600982666, -6.140598297119141, -6.897054195404053, -6.037844657897949, -6.310939788818359, -6.9790449142456055, -7.31306266784668, -6.235873699188232, -6.974239349365234, -7.135892868041992, -5.786962509155273, -6.944798946380615, -6.641756534576416, -6.693358421325684, -4.95639181137085, -5.660061359405518, -5.853227138519287, -5.477502822875977, -6.687269687652588, -5.412539482116699, -6.494658946990967, -6.931164741516113, -5.391027450561523, -5.9749040603637695, -6.983095645904541, -6.311262130737305, -6.962103843688965, -6.678888320922852, -4.228265285491943, -6.186662197113037, -5.7955121994018555, -1.4967620372772217, -1.1304409503936768, -5.920494079589844, -6.200220108032227, -6.23874044418335, -6.385483741760254, -4.709266662597656, 0.47502049803733826, -3.6770777702331543, -7.7353339195251465, -7.722665309906006, -7.727158546447754, -7.718525409698486, -7.707977771759033, -7.724466800689697, -7.729794979095459, -7.739760875701904, -7.749082565307617, -7.7447662353515625, -7.741550445556641, -7.735922813415527, -7.7301926612854, -7.7257795333862305, -7.720285415649414, -7.714573860168457, -7.680127143859863, -7.714174270629883, -7.708942890167236, -7.729074954986572, -7.730720043182373, -7.7640509605407715, -7.7366790771484375, -7.707544326782227, -7.7478861808776855, -7.743295669555664, -7.729615211486816, -7.73261022567749, -7.70119571685791, -7.7109785079956055, -7.734206199645996, -7.7134881019592285, -7.721198558807373, -7.72618293762207, -7.728708744049072, -7.741924285888672, -7.7583794593811035, -7.749011993408203, -7.754790782928467, -7.7560014724731445, -7.752836227416992, -7.7492146492004395, -7.733428478240967, -7.744163513183594, -7.747539520263672, -7.6951751708984375, -7.718990802764893, -7.700222015380859, -7.651040554046631, -7.714547634124756, -7.655431270599365, -7.61740779876709, -7.597021102905273, -7.610077381134033, -7.694004058837891, -7.591256141662598, -7.675059795379639, -7.643812656402588, -7.6856184005737305, -7.712052822113037, -7.707551002502441, -7.723432540893555, -7.719368934631348, -7.735681533813477, -7.7324676513671875, -7.704407215118408, -7.736728191375732, -7.722037315368652, -7.722352027893066, -7.731113433837891, -7.7390522956848145, -7.723179340362549, -7.718828201293945, -7.703695297241211, -7.724565029144287, -7.734192371368408, -7.751098155975342, -7.743480682373047, -7.719545841217041, -7.716937065124512, -7.7123308181762695, -7.743597507476807, -7.752394676208496, -7.7512078285217285, -7.732945442199707, -7.725142955780029, -7.741260528564453, -7.7411603927612305, -7.737780570983887, -7.750965595245361, -7.740818977355957, -7.686964988708496, -7.713306903839111, -7.741178512573242, -7.742282390594482, -7.726012229919434, -7.744663238525391, -7.725052833557129, -7.733858108520508, -7.740671157836914, -7.736743450164795, -7.724300384521484, -7.730321884155273, -7.736137866973877, -7.70383882522583, -7.683542251586914, -7.718855857849121, -7.686707973480225, -7.704432487487793, -7.658102989196777, -7.695568561553955, -7.69036865234375, -7.747773170471191, -7.7402143478393555, -7.742645740509033, -7.763980865478516, -7.7680983543396, -7.740753650665283, -7.743127822875977, -7.7349748611450195, -7.745551586151123, -7.738755702972412, -7.741732120513916, -7.748137474060059, -7.743581295013428, -7.735670566558838, -7.764667510986328, -7.710698127746582, -7.739935874938965, -7.722173690795898, -7.706691741943359, -7.663769245147705, -7.616824626922607, -7.639028549194336, -7.677547454833984, -7.650203704833984, -7.696557998657227, -7.684380054473877, -7.699834823608398, -7.750391006469727, -7.738962173461914, -7.702470302581787, -7.729560375213623, -7.723387718200684, -7.69634485244751, -7.729039669036865, -7.721960067749023, -7.692709445953369, -7.6708292961120605, -7.698899269104004, -7.7314863204956055, -7.723443031311035, -7.708186149597168, -7.717802047729492, -7.731648921966553, -7.736522674560547, -7.7150726318359375, -7.7259063720703125, -7.719644546508789, -7.7291765213012695, -7.729486465454102, -7.729917526245117, -7.722917556762695, -7.731058120727539, -7.742368698120117, -7.721638202667236, -7.707164287567139, -7.733569145202637, -7.744810581207275, -7.746173858642578, -7.747046947479248, -7.720710754394531, -7.7087812423706055, -7.678387641906738, -7.686713695526123, -7.728318691253662, -7.7119245529174805, -7.754678726196289, -7.760128974914551, -7.727015495300293, -7.741124153137207, -7.733457088470459, -7.734555244445801, -7.731627941131592, -7.721691608428955, -7.7414069175720215, -7.737066745758057, -7.70165491104126, -7.682351112365723, -7.738255500793457, -7.7053680419921875, -7.707165718078613, -7.740077972412109, -7.724494934082031, -7.740604400634766, -7.740170001983643, -7.7462382316589355, -7.699326515197754, -7.726251602172852, -7.718468189239502, -7.716907501220703, -7.670829772949219, -7.7080512046813965, -7.730978012084961, -7.720769882202148, -7.727847099304199, -7.720287799835205, -7.723450183868408, -7.725756645202637, -7.736851692199707, -7.720353603363037, -7.764747619628906, -7.742897987365723, -7.741298198699951, -7.703051567077637, -7.739587306976318, -7.699265480041504, -7.6991119384765625, -7.700871467590332, -7.646491050720215, -7.674882888793945, -7.685094356536865, -7.613667964935303, -7.679942607879639, -7.693460941314697, -7.649313449859619, -7.669244289398193, -7.645094394683838, -7.7025675773620605, -7.711365699768066, -7.731800079345703, -7.740283966064453, -7.726409435272217, -7.7300615310668945, -7.752629280090332, -7.750398635864258, -7.708598613739014, -7.737283706665039, -7.7395477294921875, -7.716788291931152, -7.7085418701171875, -7.719415187835693, -7.7277045249938965, -7.728724002838135, -7.723803997039795, -7.684690952301025, -7.69986629486084, -7.699082374572754, -7.712274074554443, -7.712960720062256, -7.701467037200928, -7.7078046798706055, -7.702426433563232, -7.7083563804626465, -7.70988655090332, -7.718352794647217, -7.719523906707764, -7.7272844314575195, -7.739331245422363, -7.728926658630371, -7.713949680328369, -7.697629928588867, -7.690842628479004, -7.7183966636657715, -7.723289489746094, -7.721104145050049, -7.7307305335998535, -7.718273639678955, -7.72764253616333, -7.724047660827637, -7.737951278686523, -7.74037504196167, -7.729394912719727, -7.7249531745910645, -7.759939670562744]\n",
            "[-1.4400299787521362]\n",
            "[-2.474254608154297]\n",
            "[0.8113058805465698]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simplified inference\n",
        "\n",
        "Since, the span is the best answer, our prediction should be the span."
      ],
      "metadata": {
        "id": "-34Q8AzjBMjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = [np.amax(start_logits)+np.amax(end_logits), yes_logits, no_logits, unk_logits]\n",
        "print(_compute_softmax(result))\n",
        "best_span_start = np.argmax(start_logits)\n",
        "best_span_end = np.argmax(end_logits)\n",
        "\n",
        "print(\"The answer is : \")\n",
        "print(\" \".join(my_features.tokens[best_span_start:best_span_end+1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZJ5sULfBTDb",
        "outputId": "87aca41e-fe49-4f75-fbd7-8469ef2868ce"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9999981877234422, 1.6693900411397654e-07, 5.934713645617118e-08, 1.5859904173789526e-06]\n",
            "The answer is : \n",
            "dr . samuel reynolds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our system seems to be working perfectly and produces the right answer. Trace back your steps and try with your own questions, or stories."
      ],
      "metadata": {
        "id": "BpuYDBU7CB-P"
      }
    }
  ]
}